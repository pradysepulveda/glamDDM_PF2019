{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(23) # from random.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1. Hierarchical GLAM estimation and out of sample prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>trial</th>\n",
       "      <th>choice</th>\n",
       "      <th>rt</th>\n",
       "      <th>item_value_0</th>\n",
       "      <th>item_value_1</th>\n",
       "      <th>gaze_0</th>\n",
       "      <th>gaze_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1734.284</td>\n",
       "      <td>110</td>\n",
       "      <td>131</td>\n",
       "      <td>0.669090</td>\n",
       "      <td>0.330910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6555.370</td>\n",
       "      <td>47</td>\n",
       "      <td>50</td>\n",
       "      <td>0.759630</td>\n",
       "      <td>0.240370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3174.566</td>\n",
       "      <td>50</td>\n",
       "      <td>44</td>\n",
       "      <td>0.549371</td>\n",
       "      <td>0.450629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2877.579</td>\n",
       "      <td>57</td>\n",
       "      <td>50</td>\n",
       "      <td>0.608409</td>\n",
       "      <td>0.391591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1806.310</td>\n",
       "      <td>42</td>\n",
       "      <td>50</td>\n",
       "      <td>0.522849</td>\n",
       "      <td>0.477151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject  trial  choice        rt  item_value_0  item_value_1    gaze_0  \\\n",
       "0        1      0       1  1734.284           110           131  0.669090   \n",
       "1        1      1       0  6555.370            47            50  0.759630   \n",
       "2        1      2       0  3174.566            50            44  0.549371   \n",
       "3        1      3       1  2877.579            57            50  0.608409   \n",
       "4        1      4       1  1806.310            42            50  0.522849   \n",
       "\n",
       "     gaze_1  \n",
       "0  0.330910  \n",
       "1  0.240370  \n",
       "2  0.450629  \n",
       "3  0.391591  \n",
       "4  0.477151  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "sufix = 'hierarchical_More_NoBin_Gamma-11_NUTS_33'\n",
    "data = pd.read_csv('data/PF2019_data/GlamDataPF2019_More_NoBin_33.csv')\n",
    "#data = pd.read_csv('data/PF2019_data/GlamDataFF2018_Like_NoBin.csv')\n",
    "\n",
    "# Subset only necessary columns\n",
    "data = data[['subject', 'trial', 'choice', 'rt',\n",
    "         'item_value_0', 'item_value_1',\n",
    "         'gaze_0', 'gaze_1']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data in training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data into training (1920 trials) and test (1920 trials) sets...\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.DataFrame()\n",
    "test_data = pd.DataFrame()\n",
    "\n",
    "for subject in data.subject.unique():\n",
    "    subject_data = data[data['subject'] == subject].copy().reset_index(drop=True)\n",
    "    n_trials = len(subject_data)\n",
    "    \n",
    "    subject_train = subject_data.iloc[np.arange(0, n_trials, 2)].copy()\n",
    "    subject_test = subject_data.iloc[np.arange(1, n_trials, 2)].copy()\n",
    "\n",
    "    test_data = pd.concat([test_data, subject_test])\n",
    "    train_data = pd.concat([train_data, subject_train])\n",
    "\n",
    "test_data.to_csv(str('data/PF2019_data/GlamDataPF2019_preprocessed_test'+sufix+'.csv'))\n",
    "train_data.to_csv(str('data/PF2019_data/GlamDataPF2019_preprocessed_train'+sufix+'.csv'))\n",
    "\n",
    "print('Split data into training ({} trials) and test ({} trials) sets...'.format(len(train_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>trial</th>\n",
       "      <th>choice</th>\n",
       "      <th>rt</th>\n",
       "      <th>item_value_0</th>\n",
       "      <th>item_value_1</th>\n",
       "      <th>gaze_0</th>\n",
       "      <th>gaze_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1734.284</td>\n",
       "      <td>110</td>\n",
       "      <td>131</td>\n",
       "      <td>0.669090</td>\n",
       "      <td>0.330910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3174.566</td>\n",
       "      <td>50</td>\n",
       "      <td>44</td>\n",
       "      <td>0.549371</td>\n",
       "      <td>0.450629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1806.310</td>\n",
       "      <td>42</td>\n",
       "      <td>50</td>\n",
       "      <td>0.522849</td>\n",
       "      <td>0.477151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3650.266</td>\n",
       "      <td>78</td>\n",
       "      <td>80</td>\n",
       "      <td>0.682034</td>\n",
       "      <td>0.317966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1259.268</td>\n",
       "      <td>50</td>\n",
       "      <td>48</td>\n",
       "      <td>0.508019</td>\n",
       "      <td>0.491981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5698.027</td>\n",
       "      <td>46</td>\n",
       "      <td>50</td>\n",
       "      <td>0.582089</td>\n",
       "      <td>0.417911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2626.168</td>\n",
       "      <td>50</td>\n",
       "      <td>56</td>\n",
       "      <td>0.666353</td>\n",
       "      <td>0.333647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1671.149</td>\n",
       "      <td>110</td>\n",
       "      <td>124</td>\n",
       "      <td>0.508035</td>\n",
       "      <td>0.491965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1527.826</td>\n",
       "      <td>64</td>\n",
       "      <td>80</td>\n",
       "      <td>0.473512</td>\n",
       "      <td>0.526488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2267.665</td>\n",
       "      <td>101</td>\n",
       "      <td>110</td>\n",
       "      <td>0.553242</td>\n",
       "      <td>0.446758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>5571.061</td>\n",
       "      <td>50</td>\n",
       "      <td>42</td>\n",
       "      <td>0.575306</td>\n",
       "      <td>0.424694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1402.112</td>\n",
       "      <td>77</td>\n",
       "      <td>80</td>\n",
       "      <td>0.435523</td>\n",
       "      <td>0.564477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2065.604</td>\n",
       "      <td>44</td>\n",
       "      <td>50</td>\n",
       "      <td>0.528335</td>\n",
       "      <td>0.471665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>1613.454</td>\n",
       "      <td>50</td>\n",
       "      <td>46</td>\n",
       "      <td>0.485035</td>\n",
       "      <td>0.514965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>4895.517</td>\n",
       "      <td>133</td>\n",
       "      <td>110</td>\n",
       "      <td>0.698379</td>\n",
       "      <td>0.301621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>2000.688</td>\n",
       "      <td>126</td>\n",
       "      <td>110</td>\n",
       "      <td>0.511292</td>\n",
       "      <td>0.488708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>2050.497</td>\n",
       "      <td>110</td>\n",
       "      <td>126</td>\n",
       "      <td>0.522393</td>\n",
       "      <td>0.477607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1635.473</td>\n",
       "      <td>80</td>\n",
       "      <td>77</td>\n",
       "      <td>0.522261</td>\n",
       "      <td>0.477739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>4443.018</td>\n",
       "      <td>80</td>\n",
       "      <td>64</td>\n",
       "      <td>0.567061</td>\n",
       "      <td>0.432939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>1725.903</td>\n",
       "      <td>124</td>\n",
       "      <td>110</td>\n",
       "      <td>0.558840</td>\n",
       "      <td>0.441160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1686.530</td>\n",
       "      <td>128</td>\n",
       "      <td>110</td>\n",
       "      <td>0.657603</td>\n",
       "      <td>0.342397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>2244.929</td>\n",
       "      <td>54</td>\n",
       "      <td>50</td>\n",
       "      <td>0.702233</td>\n",
       "      <td>0.297767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>1361.319</td>\n",
       "      <td>105</td>\n",
       "      <td>110</td>\n",
       "      <td>0.482747</td>\n",
       "      <td>0.517253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>2850.121</td>\n",
       "      <td>96</td>\n",
       "      <td>80</td>\n",
       "      <td>0.698865</td>\n",
       "      <td>0.301135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>1391.672</td>\n",
       "      <td>110</td>\n",
       "      <td>128</td>\n",
       "      <td>0.452632</td>\n",
       "      <td>0.547368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1582.340</td>\n",
       "      <td>110</td>\n",
       "      <td>119</td>\n",
       "      <td>0.559501</td>\n",
       "      <td>0.440499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>2184.782</td>\n",
       "      <td>58</td>\n",
       "      <td>50</td>\n",
       "      <td>0.581910</td>\n",
       "      <td>0.418090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>2890.742</td>\n",
       "      <td>74</td>\n",
       "      <td>80</td>\n",
       "      <td>0.530453</td>\n",
       "      <td>0.469547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1901.984</td>\n",
       "      <td>66</td>\n",
       "      <td>80</td>\n",
       "      <td>0.418559</td>\n",
       "      <td>0.581441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>2545.418</td>\n",
       "      <td>94</td>\n",
       "      <td>110</td>\n",
       "      <td>0.576064</td>\n",
       "      <td>0.423936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>33</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>4251.426</td>\n",
       "      <td>75</td>\n",
       "      <td>80</td>\n",
       "      <td>0.503628</td>\n",
       "      <td>0.496372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>33</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>4601.336</td>\n",
       "      <td>89</td>\n",
       "      <td>110</td>\n",
       "      <td>0.477015</td>\n",
       "      <td>0.522985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>33</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>3649.435</td>\n",
       "      <td>110</td>\n",
       "      <td>98</td>\n",
       "      <td>0.416827</td>\n",
       "      <td>0.583173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>33</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>3708.628</td>\n",
       "      <td>110</td>\n",
       "      <td>94</td>\n",
       "      <td>0.451137</td>\n",
       "      <td>0.548863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>33</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>5089.783</td>\n",
       "      <td>50</td>\n",
       "      <td>54</td>\n",
       "      <td>0.375723</td>\n",
       "      <td>0.624277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>33</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>6969.709</td>\n",
       "      <td>50</td>\n",
       "      <td>52</td>\n",
       "      <td>0.424717</td>\n",
       "      <td>0.575283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>33</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>1858.269</td>\n",
       "      <td>87</td>\n",
       "      <td>110</td>\n",
       "      <td>0.469035</td>\n",
       "      <td>0.530965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>33</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>3910.930</td>\n",
       "      <td>80</td>\n",
       "      <td>96</td>\n",
       "      <td>0.460076</td>\n",
       "      <td>0.539924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>33</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>2087.100</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>0.532287</td>\n",
       "      <td>0.467713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>33</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>4085.180</td>\n",
       "      <td>45</td>\n",
       "      <td>50</td>\n",
       "      <td>0.425232</td>\n",
       "      <td>0.574768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>33</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>6549.821</td>\n",
       "      <td>50</td>\n",
       "      <td>53</td>\n",
       "      <td>0.568922</td>\n",
       "      <td>0.431078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>33</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>4550.765</td>\n",
       "      <td>80</td>\n",
       "      <td>88</td>\n",
       "      <td>0.449738</td>\n",
       "      <td>0.550262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>33</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>5756.064</td>\n",
       "      <td>103</td>\n",
       "      <td>110</td>\n",
       "      <td>0.479955</td>\n",
       "      <td>0.520045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>33</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>4035.357</td>\n",
       "      <td>80</td>\n",
       "      <td>72</td>\n",
       "      <td>0.477174</td>\n",
       "      <td>0.522826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>33</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>1844.621</td>\n",
       "      <td>80</td>\n",
       "      <td>90</td>\n",
       "      <td>0.497924</td>\n",
       "      <td>0.502076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>33</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>3936.449</td>\n",
       "      <td>80</td>\n",
       "      <td>70</td>\n",
       "      <td>0.477459</td>\n",
       "      <td>0.522541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>33</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>11964.377</td>\n",
       "      <td>82</td>\n",
       "      <td>80</td>\n",
       "      <td>0.484599</td>\n",
       "      <td>0.515401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>33</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>2743.440</td>\n",
       "      <td>96</td>\n",
       "      <td>110</td>\n",
       "      <td>0.529970</td>\n",
       "      <td>0.470030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>33</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>2532.466</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>0.539153</td>\n",
       "      <td>0.460847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>33</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>6412.806</td>\n",
       "      <td>110</td>\n",
       "      <td>96</td>\n",
       "      <td>0.513330</td>\n",
       "      <td>0.486670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>33</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>4007.788</td>\n",
       "      <td>49</td>\n",
       "      <td>50</td>\n",
       "      <td>0.557839</td>\n",
       "      <td>0.442161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>33</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>3685.316</td>\n",
       "      <td>51</td>\n",
       "      <td>50</td>\n",
       "      <td>0.582175</td>\n",
       "      <td>0.417825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>33</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>1988.182</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>0.399461</td>\n",
       "      <td>0.600539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>33</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>4300.777</td>\n",
       "      <td>85</td>\n",
       "      <td>80</td>\n",
       "      <td>0.492486</td>\n",
       "      <td>0.507514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>33</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>7640.657</td>\n",
       "      <td>50</td>\n",
       "      <td>43</td>\n",
       "      <td>0.593629</td>\n",
       "      <td>0.406371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>33</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>3341.929</td>\n",
       "      <td>50</td>\n",
       "      <td>60</td>\n",
       "      <td>0.476386</td>\n",
       "      <td>0.523614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>33</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>2674.219</td>\n",
       "      <td>122</td>\n",
       "      <td>110</td>\n",
       "      <td>0.584644</td>\n",
       "      <td>0.415356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>33</td>\n",
       "      <td>114</td>\n",
       "      <td>1</td>\n",
       "      <td>2848.448</td>\n",
       "      <td>80</td>\n",
       "      <td>67</td>\n",
       "      <td>0.447141</td>\n",
       "      <td>0.552859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>33</td>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "      <td>2835.649</td>\n",
       "      <td>110</td>\n",
       "      <td>122</td>\n",
       "      <td>0.680958</td>\n",
       "      <td>0.319042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>33</td>\n",
       "      <td>118</td>\n",
       "      <td>1</td>\n",
       "      <td>3903.836</td>\n",
       "      <td>110</td>\n",
       "      <td>112</td>\n",
       "      <td>0.392389</td>\n",
       "      <td>0.607611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1920 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject  trial  choice         rt  item_value_0  item_value_1    gaze_0  \\\n",
       "0          1      0       1   1734.284           110           131  0.669090   \n",
       "2          1      2       0   3174.566            50            44  0.549371   \n",
       "4          1      4       1   1806.310            42            50  0.522849   \n",
       "6          1      6       1   3650.266            78            80  0.682034   \n",
       "8          1      8       1   1259.268            50            48  0.508019   \n",
       "10         1     10       1   5698.027            46            50  0.582089   \n",
       "12         1     12       1   2626.168            50            56  0.666353   \n",
       "14         1     14       1   1671.149           110           124  0.508035   \n",
       "16         1     16       1   1527.826            64            80  0.473512   \n",
       "18         1     18       1   2267.665           101           110  0.553242   \n",
       "20         1     20       0   5571.061            50            42  0.575306   \n",
       "22         1     22       1   1402.112            77            80  0.435523   \n",
       "24         1     24       1   2065.604            44            50  0.528335   \n",
       "26         1     26       1   1613.454            50            46  0.485035   \n",
       "28         1     28       0   4895.517           133           110  0.698379   \n",
       "30         1     30       1   2000.688           126           110  0.511292   \n",
       "32         1     32       1   2050.497           110           126  0.522393   \n",
       "34         1     34       1   1635.473            80            77  0.522261   \n",
       "36         1     36       0   4443.018            80            64  0.567061   \n",
       "38         1     38       1   1725.903           124           110  0.558840   \n",
       "40         1     40       0   1686.530           128           110  0.657603   \n",
       "42         1     42       0   2244.929            54            50  0.702233   \n",
       "44         1     44       1   1361.319           105           110  0.482747   \n",
       "46         1     46       0   2850.121            96            80  0.698865   \n",
       "48         1     48       1   1391.672           110           128  0.452632   \n",
       "50         1     50       1   1582.340           110           119  0.559501   \n",
       "52         1     52       0   2184.782            58            50  0.581910   \n",
       "54         1     54       1   2890.742            74            80  0.530453   \n",
       "56         1     56       1   1901.984            66            80  0.418559   \n",
       "58         1     58       1   2545.418            94           110  0.576064   \n",
       "..       ...    ...     ...        ...           ...           ...       ...   \n",
       "60        33     60       1   4251.426            75            80  0.503628   \n",
       "62        33     62       0   4601.336            89           110  0.477015   \n",
       "64        33     64       0   3649.435           110            98  0.416827   \n",
       "66        33     66       0   3708.628           110            94  0.451137   \n",
       "68        33     68       1   5089.783            50            54  0.375723   \n",
       "70        33     70       1   6969.709            50            52  0.424717   \n",
       "72        33     72       1   1858.269            87           110  0.469035   \n",
       "74        33     74       1   3910.930            80            96  0.460076   \n",
       "76        33     76       1   2087.100            50            55  0.532287   \n",
       "78        33     78       1   4085.180            45            50  0.425232   \n",
       "80        33     80       1   6549.821            50            53  0.568922   \n",
       "82        33     82       1   4550.765            80            88  0.449738   \n",
       "84        33     84       0   5756.064           103           110  0.479955   \n",
       "86        33     86       0   4035.357            80            72  0.477174   \n",
       "88        33     88       1   1844.621            80            90  0.497924   \n",
       "90        33     90       0   3936.449            80            70  0.477459   \n",
       "92        33     92       0  11964.377            82            80  0.484599   \n",
       "94        33     94       1   2743.440            96           110  0.529970   \n",
       "96        33     96       0   2532.466            60            50  0.539153   \n",
       "98        33     98       1   6412.806           110            96  0.513330   \n",
       "100       33    100       0   4007.788            49            50  0.557839   \n",
       "102       33    102       0   3685.316            51            50  0.582175   \n",
       "104       33    104       1   1988.182            40            50  0.399461   \n",
       "106       33    106       0   4300.777            85            80  0.492486   \n",
       "108       33    108       0   7640.657            50            43  0.593629   \n",
       "110       33    110       1   3341.929            50            60  0.476386   \n",
       "112       33    112       0   2674.219           122           110  0.584644   \n",
       "114       33    114       1   2848.448            80            67  0.447141   \n",
       "116       33    116       1   2835.649           110           122  0.680958   \n",
       "118       33    118       1   3903.836           110           112  0.392389   \n",
       "\n",
       "       gaze_1  \n",
       "0    0.330910  \n",
       "2    0.450629  \n",
       "4    0.477151  \n",
       "6    0.317966  \n",
       "8    0.491981  \n",
       "10   0.417911  \n",
       "12   0.333647  \n",
       "14   0.491965  \n",
       "16   0.526488  \n",
       "18   0.446758  \n",
       "20   0.424694  \n",
       "22   0.564477  \n",
       "24   0.471665  \n",
       "26   0.514965  \n",
       "28   0.301621  \n",
       "30   0.488708  \n",
       "32   0.477607  \n",
       "34   0.477739  \n",
       "36   0.432939  \n",
       "38   0.441160  \n",
       "40   0.342397  \n",
       "42   0.297767  \n",
       "44   0.517253  \n",
       "46   0.301135  \n",
       "48   0.547368  \n",
       "50   0.440499  \n",
       "52   0.418090  \n",
       "54   0.469547  \n",
       "56   0.581441  \n",
       "58   0.423936  \n",
       "..        ...  \n",
       "60   0.496372  \n",
       "62   0.522985  \n",
       "64   0.583173  \n",
       "66   0.548863  \n",
       "68   0.624277  \n",
       "70   0.575283  \n",
       "72   0.530965  \n",
       "74   0.539924  \n",
       "76   0.467713  \n",
       "78   0.574768  \n",
       "80   0.431078  \n",
       "82   0.550262  \n",
       "84   0.520045  \n",
       "86   0.522826  \n",
       "88   0.502076  \n",
       "90   0.522541  \n",
       "92   0.515401  \n",
       "94   0.470030  \n",
       "96   0.460847  \n",
       "98   0.486670  \n",
       "100  0.442161  \n",
       "102  0.417825  \n",
       "104  0.600539  \n",
       "106  0.507514  \n",
       "108  0.406371  \n",
       "110  0.523614  \n",
       "112  0.415356  \n",
       "114  0.552859  \n",
       "116  0.319042  \n",
       "118  0.607611  \n",
       "\n",
       "[1920 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical GLAM estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. full GLAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting full GLAM hierarchically...\n",
      "Generating single subject models for 32 subjects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 32 model(s) using NUTS...\n",
      "  Fitting model 1 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:11<00:00, 1038.95draws/s]\n",
      "There were 2 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 2 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:18<00:00, 651.95draws/s]\n",
      "There were 47 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 43 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 50 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 43 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 3 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:14<00:00, 830.78draws/s]\n",
      "There were 123 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 185 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 138 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 147 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 4 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:14<00:00, 817.96draws/s]\n",
      "There were 41 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 28 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 40 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 40 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 5 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:12<00:00, 943.83draws/s]\n",
      "There were 2 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 6 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:14<00:00, 818.01draws/s]\n",
      "There were 3 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.8799485849873582, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "There were 6 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 8 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.8981774391494725, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 7 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:18<00:00, 646.57draws/s]\n",
      "There were 13 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 2 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 2 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 8 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:13<00:00, 881.33draws/s]\n",
      "There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 9 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:11<00:00, 1014.74draws/s]\n",
      "There were 35 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 37 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 24 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 32 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 10 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:12<00:00, 945.76draws/s]\n",
      "There were 2 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 21 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.6988306481239942, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "There were 2 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 11 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:13<00:00, 890.26draws/s]\n",
      "There were 158 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 138 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 131 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 136 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 12 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:11<00:00, 1075.98draws/s]\n",
      "The acceptance probability does not match the target. It is 0.8849509223576897, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The acceptance probability does not match the target. It is 0.6966123193269532, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 13 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [11:50<00:00,  3.88draws/s]\n",
      "There were 38 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.652840423067064, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "There were 4 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.8799764671095479, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "There were 26 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The gelman-rubin statistic is larger than 1.4 for some parameters. The sampler did not converge.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 14 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:11<00:00, 1064.56draws/s]\n",
      "There were 101 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 148 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 76 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 97 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 15 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:16<00:00, 711.24draws/s]\n",
      "There were 100 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 62 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 93 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 60 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 16 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:25<00:00, 467.16draws/s]\n",
      "There were 101 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.9100193217790833, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "There were 449 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.558953569611484, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "There were 203 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.5587931752794926, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "There were 111 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 17 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:12<00:00, 943.84draws/s]\n",
      "There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.8794101178300339, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "There were 8 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 2 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 5 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 18 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:09<00:00, 1210.71draws/s]\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 19 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:30<00:00, 398.73draws/s]\n",
      "There were 94 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.6525463358732884, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "There were 18 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 21 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.9356809315434799, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "There were 62 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.6896103745502107, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 20 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:20<00:00, 597.15draws/s]\n",
      "There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 4 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 2 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 21 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:11<00:00, 1040.17draws/s]\n",
      "There were 12 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 21 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.896292655029272, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "There were 21 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 51 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 22 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:13<00:00, 875.57draws/s]\n",
      "There were 1243 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 1189 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 1098 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 1272 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 23 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:12<00:00, 962.33draws/s]\n",
      "There were 2 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.8871559125146787, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 24 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:15<00:00, 791.67draws/s]\n",
      "There were 21 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 21 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 14 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 12 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 25 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:11<00:00, 1003.10draws/s]\n",
      "There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 8 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 13 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 5 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 26 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:16<00:00, 711.77draws/s]\n",
      "There were 10 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 8 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 10 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 12 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 27 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:12<00:00, 949.41draws/s]\n",
      "There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 5 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.9024634839476384, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "There were 2 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.8907039117862787, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 28 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [16:02<00:00,  7.14draws/s]\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.9170981570841957, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The gelman-rubin statistic is larger than 1.4 for some parameters. The sampler did not converge.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 29 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:39<00:00, 306.61draws/s]\n",
      "There were 5 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 4 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 30 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:13<00:00, 904.52draws/s]\n",
      "There were 66 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 27 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 25 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 55 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 31 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:13<00:00, 917.08draws/s]\n",
      "There were 8 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 93 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 13 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 15 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.4775538612458078, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 32 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:15<00:00, 761.01draws/s]\n",
      "There were 65 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 48 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 54 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 103 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/!\\ Automatically setting parameter precision...\n"
     ]
    }
   ],
   "source": [
    "# Fitting full GLAM\n",
    "print('Fitting full GLAM hierarchically...')\n",
    "\n",
    "glam_full = glam.GLAM(train_data)\n",
    "\n",
    "if not os.path.exists(str('results/estimates/glam_PF2019_full_hierarchical_cv'+sufix+'.npy')):\n",
    "    glam_full.make_model('individual', gamma_bounds=(-1, 1), t0_val=0)\n",
    "    glam_full.fit(method='NUTS', tune=1000)\n",
    "else:\n",
    "    print('  Found old parameter estimates in \"results/estimates\". Skipping estimation...')\n",
    "    glam_full.estimates = np.load(str('results/estimates/glam_PF2019_full_hierarchical_cv'+sufix+'.npy'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNR</th>\n",
       "      <th>b</th>\n",
       "      <th>gamma</th>\n",
       "      <th>p_error</th>\n",
       "      <th>s</th>\n",
       "      <th>t0</th>\n",
       "      <th>tau</th>\n",
       "      <th>v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>203.91</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>484.37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.005238</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300.85</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007449</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>462.96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.010301</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>173.23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.005436</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>152.61</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007799</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>404.11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.011048</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>123.62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007531</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>54.69</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.005685</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>185.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.009549</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>441.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008549</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>105.35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006590</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>82.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>266.77</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008386</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.000038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>365.90</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008272</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>150.61</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008647</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>4.40</td>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>369.29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006765</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>202.81</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>182.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007874</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>337.09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>4.10</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>271.43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007451</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>299.06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.012091</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.000044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>212.74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006770</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>499.98</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008297</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>348.53</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>4.09</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>138.15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006002</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.000034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>194.02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006639</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.000036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>55.06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.000084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>97.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.005154</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>196.77</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008771</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.000044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>266.48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007490</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>341.66</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SNR    b  gamma  p_error         s     t0   tau         v\n",
       "0   203.91  1.0   0.85     0.05  0.009766  [0.0]  0.28  0.000041\n",
       "1   484.37  1.0   0.89     0.05  0.005238  [0.0]  0.29  0.000012\n",
       "2   300.85  1.0  -0.09     0.05  0.007449  [0.0]  0.13  0.000031\n",
       "3   462.96  1.0   0.98     0.05  0.010301  [0.0]  0.17  0.000022\n",
       "4   173.23  1.0   0.68     0.05  0.005436  [0.0]  0.13  0.000031\n",
       "5   152.61  1.0   0.63     0.05  0.007799  [0.0]  0.08  0.000043\n",
       "6   404.11  1.0   0.82     0.05  0.011048  [0.0]  0.10  0.000025\n",
       "7   123.62  1.0  -0.27     0.05  0.007531  [0.0]  0.06  0.000052\n",
       "8    54.69  1.0   0.99     0.05  0.005685  [0.0]  0.02  0.000095\n",
       "9   185.80  1.0   0.08     0.05  0.009549  [0.0]  0.32  0.000040\n",
       "10  441.54  1.0   0.70     0.05  0.008549  [0.0]  0.11  0.000020\n",
       "11  105.35  1.0  -0.44     0.05  0.006590  [0.0]  0.10  0.000061\n",
       "12   82.30  1.0   0.96     0.05  0.001909  [0.0]  0.03  0.000019\n",
       "13  266.77  1.0   0.44     0.05  0.008386  [0.0]  0.27  0.000038\n",
       "14  365.90  1.0   0.78     0.05  0.008272  [0.0]  0.13  0.000023\n",
       "15  150.61  1.0   0.59     0.05  0.008647  [0.0]  4.40  0.000046\n",
       "16  369.29  1.0   0.99     0.05  0.006765  [0.0]  0.32  0.000017\n",
       "17  202.81  1.0   0.51     0.05  0.004763  [0.0]  0.13  0.000027\n",
       "18  182.30  1.0   0.59     0.05  0.007874  [0.0]  0.72  0.000039\n",
       "19  337.09  1.0   0.58     0.05  0.010811  [0.0]  4.10  0.000027\n",
       "20  271.43  1.0   0.60     0.05  0.007451  [0.0]  0.11  0.000039\n",
       "21  299.06  1.0   0.12     0.05  0.012091  [0.0]  1.38  0.000044\n",
       "22  212.74  1.0   0.65     0.05  0.006770  [0.0]  0.21  0.000030\n",
       "23  499.98  1.0   0.71     0.05  0.008297  [0.0]  0.20  0.000020\n",
       "24  348.53  1.0   0.94     0.05  0.006249  [0.0]  4.09  0.000017\n",
       "25  138.15  1.0   0.42     0.05  0.006002  [0.0]  0.08  0.000034\n",
       "26  194.02  1.0   0.56     0.05  0.006639  [0.0]  0.07  0.000036\n",
       "27   55.06  1.0   0.40     0.05  0.004388  [0.0]  0.03  0.000084\n",
       "28   97.50  1.0   0.17     0.05  0.005154  [0.0]  0.13  0.000057\n",
       "29  196.77  1.0   0.58     0.05  0.008771  [0.0]  0.20  0.000044\n",
       "30  266.48  1.0   0.85     0.05  0.007490  [0.0]  0.07  0.000043\n",
       "31  341.66  1.0   0.71     0.05  0.006352  [0.0]  0.10  0.000032"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save parameter estimates\n",
    "np.save(str('results/estimates/glam_PF2019_nobias_hierarchical_cv'+sufix+'.npy'), glam_full.estimates)\n",
    "pd.DataFrame(glam_full.estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing WAIC scores for full model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "# Compute WAICs\n",
    "print('Computing WAIC scores for full model...')\n",
    "if not os.path.exists(str('results/waic/glam_PF2019_full'+ sufix +'.npy')):\n",
    "    # Note: DIC computation does not work for ADVI fitted models\n",
    "    # But we are using WAIC\n",
    "    glam_full.compute_waic()\n",
    "else:\n",
    "    print('  Found old DIC scores in \"results/waic\". Skipping WAIC computation...')\n",
    "    glam_full.waic = np.load(str('results/waic/glam_PF2019_full'+ sufix +'.npy'))\n",
    "\n",
    "# Compute WAICs\n",
    "np.save(str('results/waic/glam_PF2019_full'+ sufix +'.npy'), glam_full.waic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.04604884e+03, 0.00000000e+00, 1.72250244e+00, 1.00000000e+00],\n",
       "       [1.22067883e+03, 0.00000000e+00, 1.77895232e+00, 1.00000000e+00],\n",
       "       [1.08131950e+03, 0.00000000e+00, 4.57825376e+00, 1.00000000e+00],\n",
       "       [1.19129127e+03, 0.00000000e+00, 1.81216772e+00, 1.00000000e+00],\n",
       "       [1.09360688e+03, 0.00000000e+00, 2.09341500e+00, 1.00000000e+00],\n",
       "       [1.07141022e+03, 0.00000000e+00, 3.96698860e+00, 1.00000000e+00],\n",
       "       [1.16532728e+03, 0.00000000e+00, 1.59540281e+00, 1.00000000e+00],\n",
       "       [1.04997833e+03, 0.00000000e+00, 2.34431071e+00, 1.00000000e+00],\n",
       "       [9.92213429e+02, 0.00000000e+00, 2.57769666e+00, 1.00000000e+00],\n",
       "       [1.03406225e+03, 0.00000000e+00, 2.59437231e+00, 1.00000000e+00],\n",
       "       [1.20205192e+03, 0.00000000e+00, 3.10274965e+00, 1.00000000e+00],\n",
       "       [1.00093682e+03, 0.00000000e+00, 2.61991370e+00, 1.00000000e+00],\n",
       "       [1.05702793e+03, 0.00000000e+00, 1.21914822e+01, 1.00000000e+00],\n",
       "       [1.07529609e+03, 0.00000000e+00, 3.57531961e+00, 1.00000000e+00],\n",
       "       [1.14818307e+03, 0.00000000e+00, 2.59812852e+00, 1.00000000e+00],\n",
       "       [1.01134051e+03, 0.00000000e+00, 4.59129769e+00, 1.00000000e+00],\n",
       "       [1.17904935e+03, 0.00000000e+00, 2.61328657e+00, 1.00000000e+00],\n",
       "       [1.08212759e+03, 0.00000000e+00, 2.91359478e+00, 1.00000000e+00],\n",
       "       [1.03912684e+03, 0.00000000e+00, 2.98513902e+00, 1.00000000e+00],\n",
       "       [1.12281397e+03, 0.00000000e+00, 1.58847943e+00, 1.00000000e+00],\n",
       "       [1.08328029e+03, 0.00000000e+00, 5.13211283e+00, 1.00000000e+00],\n",
       "       [1.04048361e+03, 0.00000000e+00, 2.37669404e+00, 1.00000000e+00],\n",
       "       [1.10951177e+03, 0.00000000e+00, 3.77999267e+00, 1.00000000e+00],\n",
       "       [1.17166441e+03, 0.00000000e+00, 3.14235474e+00, 1.00000000e+00],\n",
       "       [1.14466468e+03, 0.00000000e+00, 1.24185550e+00, 1.00000000e+00],\n",
       "       [1.10702344e+03, 0.00000000e+00, 4.35968602e+00, 1.00000000e+00],\n",
       "       [1.11839910e+03, 0.00000000e+00, 3.38129679e+00, 1.00000000e+00],\n",
       "       [9.84718636e+02, 0.00000000e+00, 1.72399392e+00, 1.00000000e+00],\n",
       "       [9.37096894e+02, 0.00000000e+00, 3.91369881e+00, 1.00000000e+00],\n",
       "       [1.07135325e+03, 0.00000000e+00, 3.29260029e+00, 1.00000000e+00],\n",
       "       [1.07397668e+03, 0.00000000e+00, 6.75135179e+00, 1.00000000e+00],\n",
       "       [1.11923509e+03, 0.00000000e+00, 5.32397127e+00, 1.00000000e+00]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glam_full.waic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute LOO\n",
    "\n",
    "glam_full.loo = pm.loo(trace=glam_full.trace, model=glam_full.model)\n",
    "glam_full.loo\n",
    "np.save(str('results/loo/glam_PF2019_full'+ sufix +'.npy'), glam_full.loo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GLAM' object has no attribute 'loo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-12528b9778fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mglam_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'GLAM' object has no attribute 'loo'"
     ]
    }
   ],
   "source": [
    "glam_full.loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting test set data using full GLAM...\n",
      "Replaced attached data (1980 trials) with new data (1980 trials)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>trial</th>\n",
       "      <th>repeat</th>\n",
       "      <th>choice</th>\n",
       "      <th>rt</th>\n",
       "      <th>item_value_0</th>\n",
       "      <th>gaze_0</th>\n",
       "      <th>item_value_1</th>\n",
       "      <th>gaze_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4618.0</td>\n",
       "      <td>47</td>\n",
       "      <td>0.490772</td>\n",
       "      <td>50</td>\n",
       "      <td>0.509228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3511.0</td>\n",
       "      <td>47</td>\n",
       "      <td>0.490772</td>\n",
       "      <td>50</td>\n",
       "      <td>0.509228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3231.0</td>\n",
       "      <td>47</td>\n",
       "      <td>0.490772</td>\n",
       "      <td>50</td>\n",
       "      <td>0.509228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4558.0</td>\n",
       "      <td>47</td>\n",
       "      <td>0.490772</td>\n",
       "      <td>50</td>\n",
       "      <td>0.509228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2279.0</td>\n",
       "      <td>47</td>\n",
       "      <td>0.490772</td>\n",
       "      <td>50</td>\n",
       "      <td>0.509228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject  trial  repeat  choice      rt  item_value_0    gaze_0  \\\n",
       "0      0.0    0.0     0.0     1.0  4618.0            47  0.490772   \n",
       "1      0.0    0.0     1.0     0.0  3511.0            47  0.490772   \n",
       "2      0.0    0.0     2.0     1.0  3231.0            47  0.490772   \n",
       "3      0.0    0.0     3.0     1.0  4558.0            47  0.490772   \n",
       "4      0.0    0.0     4.0     1.0  2279.0            47  0.490772   \n",
       "\n",
       "   item_value_1    gaze_1  \n",
       "0            50  0.509228  \n",
       "1            50  0.509228  \n",
       "2            50  0.509228  \n",
       "3            50  0.509228  \n",
       "4            50  0.509228  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions\n",
    "print('Predicting test set data using full GLAM...')\n",
    "glam_full.exchange_data(test_data)\n",
    "\n",
    "if not os.path.exists(str('results/predictions/glam_PF2019_full_hierarchical_cv'+sufix+'.csv')):\n",
    "    glam_full.predict(n_repeats=50)\n",
    "    glam_full.prediction.to_csv(str('results/predictions/glam_PF2019_full_hierarchical_cv'+sufix+'.csv'), index=False)\n",
    "else:\n",
    "    print('  Found old hierarchical full GLAM predictions in \"results/predictions\". Skipping prediction...')\n",
    "    glam_full.prediction = pd.read_csv(str('results/predictions/glam_PF2019_full_hierarchical_cv'+sufix+'.csv'))\n",
    "\n",
    "glam_full.prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. no-bias GLAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting no-bias GLAM\n",
    "print('Fitting no-bias GLAM hierarchically...')\n",
    "\n",
    "glam_nobias = glam.GLAM(train_data)\n",
    "\n",
    "if not os.path.exists(str('results/estimates/glam_PF2019_nobias_hierarchical_cv'+sufix+'.npy')):\n",
    "    glam_nobias.make_model('hierarchical', gamma_val=1.0, t0_val=0)\n",
    "    glam_nobias.fit(method='NUTS', tune=1000)\n",
    "else:\n",
    "    print('  Found old parameter estimates in \"results/estimates\". Skipping estimation...')\n",
    "    glam_nobias.estimates = np.load(str('results/estimates/glam_PF2019_nobias_hierarchical_cv'+sufix+'.npy'))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# Save parameter estimates\n",
    "np.save(str('results/estimates/glam_PF2019_nobias_hierarchical_cv'+sufix+'.npy'), glam_nobias.estimates)\n",
    "pd.DataFrame(glam_nobias.estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case it is already fitted\n",
    "params_part_like = pd.DataFrame.from_dict(glam_nobias.estimates.item(0))\n",
    "params_part_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute LOO\n",
    "\n",
    "glam_nobias.loo = pm.loo(trace=glam_nobias.trace, model=glam_nobias.model)\n",
    "glam_nobias.loo\n",
    "\n",
    "np.save(str('results/loo/glam_PF2019_nobias'+ sufix +'.npy'), glam_nobias.loo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "print('Predicting test set data using no-bias GLAM...')\n",
    "glam_nobias.exchange_data(test_data)\n",
    "\n",
    "if not os.path.exists(str('results/predictions/glam_PF2019_nobias_hierarchical_cv'+sufix+'.csv')):\n",
    "    glam_nobias.predict(n_repeats=50)\n",
    "    glam_nobias.prediction.to_csv(str('results/predictions/glam_PF2019_nobias_hierarchical_cv'+sufix+'.csv'), index=False)\n",
    "else:\n",
    "    print('  Found old hierarchical no-bias GLAM predictions in \"results/predictions\". Skipping prediction...')\n",
    "    glam_nobias.prediction = pd.read_csv(str('results/predictions/glam_PF2019_nobias_hierarchical_cv'+sufix+'.csv'))\n",
    "\n",
    "glam_nobias.prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Plot fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close Figure to continue...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-45350e456e2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Close Figure to continue...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mglam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mglam_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#glam.plot_fit(test_data, [glam_full.prediction,glam_nobias.prediction]);\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GiTs/glamDDM_PF2019/glam/plots.py\u001b[0m in \u001b[0;36mplot_fit\u001b[0;34m(data, predictions, color_data)\u001b[0m\n\u001b[1;32m     13\u001b[0m     plot_rt_by_difficulty_zSc(data, predictions,\n\u001b[1;32m     14\u001b[0m                           \u001b[0mxlims\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxlabel_skip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolor_data\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                           ax=axs[0][0])\n\u001b[0m\u001b[1;32m     16\u001b[0m     plot_pleft_by_left_minus_mean_others(data, predictions,\n\u001b[1;32m     17\u001b[0m                                          xlabel_skip=5, xlims=[-3, 3], xlabel_start=0,color1 = color_data, ax=axs[0][1])\n",
      "\u001b[0;32m~/Documents/GiTs/glamDDM_PF2019/glam/plots.py\u001b[0m in \u001b[0;36mplot_rt_by_difficulty_zSc\u001b[0;34m(data, predictions, ax, xlims, xlabel_skip, color1)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscatter_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifficulty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifficulty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mposition_item\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_labels\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0mx_scatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_item\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m## ********\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Close Figure to continue...')\n",
    "glam.plot_fit(test_data, [glam_full.prediction]);\n",
    "#glam.plot_fit(test_data, [glam_full.prediction,glam_nobias.prediction]);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for full hierarchical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 5.1e-05,\n",
       "  'gamma': -0.77,\n",
       "  'SNR': 125.41,\n",
       "  's': 0.0081,\n",
       "  'tau': 0.01,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 1.6e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 290.78,\n",
       "  's': 0.006238,\n",
       "  'tau': 0.05,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 3.5e-05,\n",
       "  'gamma': -0.11,\n",
       "  'SNR': 230.96,\n",
       "  's': 0.008048,\n",
       "  'tau': 0.11,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 2.1e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 434.59,\n",
       "  's': 0.009583,\n",
       "  'tau': 0.1,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 3.2e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 165.04,\n",
       "  's': 0.005638,\n",
       "  'tau': 0.06,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 5.7e-05,\n",
       "  'gamma': -0.71,\n",
       "  'SNR': 83.7,\n",
       "  's': 0.005543,\n",
       "  'tau': 0.09,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 3e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 232.79,\n",
       "  's': 0.008216,\n",
       "  'tau': 0.08,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 5.1e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 99.14,\n",
       "  's': 0.005566,\n",
       "  'tau': 0.09,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 0.00011,\n",
       "  'gamma': -0.86,\n",
       "  'SNR': 49.13,\n",
       "  's': 0.005373,\n",
       "  'tau': 0.0,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 5.7e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 165.72,\n",
       "  's': 0.008702,\n",
       "  'tau': 0.06,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 2.8e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 326.55,\n",
       "  's': 0.008587,\n",
       "  'tau': 0.03,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 9e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 54.42,\n",
       "  's': 0.004899,\n",
       "  'tau': 0.02,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 3.4e-05,\n",
       "  'gamma': -0.56,\n",
       "  'SNR': 86.39,\n",
       "  's': 0.002529,\n",
       "  'tau': 0.0,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 4e-05,\n",
       "  'gamma': -0.87,\n",
       "  'SNR': 232.26,\n",
       "  's': 0.00942,\n",
       "  'tau': 0.25,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 4.7e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 154.16,\n",
       "  's': 0.006544,\n",
       "  'tau': 0.04,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 4.6e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 127.67,\n",
       "  's': 0.007116,\n",
       "  'tau': 0.08,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 1.9e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 476.5,\n",
       "  's': 0.008507,\n",
       "  'tau': 0.03,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 2.8e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 177.22,\n",
       "  's': 0.005505,\n",
       "  'tau': 0.05,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 3.2e-05,\n",
       "  'gamma': -0.87,\n",
       "  'SNR': 340.23,\n",
       "  's': 0.008569,\n",
       "  'tau': 0.04,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 4.5e-05,\n",
       "  'gamma': -0.98,\n",
       "  'SNR': 228.98,\n",
       "  's': 0.008275,\n",
       "  'tau': 0.03,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 3.2e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 213.49,\n",
       "  's': 0.006526,\n",
       "  'tau': 0.08,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 3.6e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 288.26,\n",
       "  's': 0.009493,\n",
       "  'tau': 0.08,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 3e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 232.13,\n",
       "  's': 0.007639,\n",
       "  'tau': 0.07,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 3.7e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 133.67,\n",
       "  's': 0.006737,\n",
       "  'tau': 0.04,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 1.7e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 336.19,\n",
       "  's': 0.007432,\n",
       "  'tau': 0.02,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 3.9e-05,\n",
       "  'gamma': -0.71,\n",
       "  'SNR': 145.41,\n",
       "  's': 0.006367,\n",
       "  'tau': 0.06,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 2.9e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 141.85,\n",
       "  's': 0.004457,\n",
       "  'tau': 0.03,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 4.6e-05,\n",
       "  'gamma': -0.98,\n",
       "  'SNR': 259.96,\n",
       "  's': 0.010083,\n",
       "  'tau': 0.18,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 6.8e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 62.69,\n",
       "  's': 0.004352,\n",
       "  'tau': 0.11,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 5.7e-05,\n",
       "  'gamma': -0.98,\n",
       "  'SNR': 115.24,\n",
       "  's': 0.008492,\n",
       "  'tau': 0.05,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 7.2e-05,\n",
       "  'gamma': 0.27,\n",
       "  'SNR': 106.53,\n",
       "  's': 0.008452,\n",
       "  'tau': 0.01,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 4.6e-05,\n",
       "  'gamma': -0.94,\n",
       "  'SNR': 145.61,\n",
       "  's': 0.006249,\n",
       "  'tau': 0.01,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 4.4e-05,\n",
       "  'gamma': -0.94,\n",
       "  'SNR': 156.31,\n",
       "  's': 0.007018,\n",
       "  'tau': 0.0,\n",
       "  't0': array([0.])}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_participant = glam_full.estimates\n",
    "params_participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-24a3f3602e89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparams_participant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglam_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mparams_participant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglam_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "params_participant = pd.DataFrame.from_dict(glam_full.estimates.item(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Mean gamma \" +  str(params_participant['gamma'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = params_participant[['SNR','gamma','tau','v']].hist(figsize = [20,3] , layout=[1,4],bins = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [END] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
