{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pradyumna/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "/Users/pradyumna/anaconda3/lib/python3.7/site-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "import glam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import arviz as az\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(23) # from random.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical GLAM estimation and out of sample prediction\n",
    "## eLife reanalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>trial</th>\n",
       "      <th>choice</th>\n",
       "      <th>rt</th>\n",
       "      <th>item_value_0</th>\n",
       "      <th>item_value_1</th>\n",
       "      <th>gaze_0</th>\n",
       "      <th>gaze_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4261.735</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.396552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3559.258</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.490772</td>\n",
       "      <td>0.509228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3754.464</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.490893</td>\n",
       "      <td>0.509107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2431.751</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.639125</td>\n",
       "      <td>0.360875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2199.342</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.702232</td>\n",
       "      <td>0.297768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject  trial  choice        rt  item_value_0  item_value_1    gaze_0  \\\n",
       "0        1      0       0  4261.735             1             0  0.603448   \n",
       "1        1      1       1  3559.258             7             7  0.490772   \n",
       "2        1      2       1  3754.464             7             7  0.490893   \n",
       "3        1      3       0  2431.751             5             7  0.639125   \n",
       "4        1      4       0  2199.342             7             7  0.702232   \n",
       "\n",
       "     gaze_1  \n",
       "0  0.396552  \n",
       "1  0.509228  \n",
       "2  0.509107  \n",
       "3  0.360875  \n",
       "4  0.297768  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "sufix = '_hierarchical_Less_Bin_Inv_Gamma-11_NUTS_33_eLife'\n",
    "data = pd.read_csv('data/PF2019_data/GlamDataPF2019_Less_Bin_Inv_33.csv')\n",
    "\n",
    "# Subset only necessary columns\n",
    "data = data[['subject', 'trial', 'choice', 'rt',\n",
    "         'item_value_0', 'item_value_1',\n",
    "         'gaze_0', 'gaze_1']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data in training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data into training (1920 trials) and test (1920 trials) sets...\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.DataFrame()\n",
    "test_data = pd.DataFrame()\n",
    "\n",
    "for subject in data.subject.unique():\n",
    "    subject_data = data[data['subject'] == subject].copy().reset_index(drop=True)\n",
    "    n_trials = len(subject_data)\n",
    "    \n",
    "    subject_train = subject_data.iloc[np.arange(0, n_trials, 2)].copy()\n",
    "    subject_test = subject_data.iloc[np.arange(1, n_trials, 2)].copy()\n",
    "\n",
    "    test_data = pd.concat([test_data, subject_test])\n",
    "    train_data = pd.concat([train_data, subject_train])\n",
    "\n",
    "#test_data.to_csv(str('data/PF2019_data/GlamDataPF2019_preprocessed_test'+sufix+'.csv'))\n",
    "#train_data.to_csv(str('data/PF2019_data/GlamDataPF2019_preprocessed_train'+sufix+'.csv'))\n",
    "\n",
    "print('Split data into training ({} trials) and test ({} trials) sets...'.format(len(train_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.subject.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.subject.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we renumber subject data for proper sequence\n",
    "train_data2 = train_data.replace(train_data.subject.unique(), list(range(len(train_data.subject.unique()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data2.subject.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical GLAM estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. full GLAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting full GLAM hierarchically...\n",
      "Generating hierarchical model for 32 subjects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pradyumna/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/Users/pradyumna/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/Users/pradyumna/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 model(s) using NUTS...\n",
      "  Fitting model 1 of 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pradyumna/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/Users/pradyumna/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, tau_sd, tau_mu, SNR, SNR_sd, SNR_mu, gamma, gamma_sd, gamma_mu, v, v_sd, v_mu]\n",
      "/Users/pradyumna/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='12000' class='' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [12000/12000 2:43:20<00:00 Sampling 4 chains, 3 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pradyumna/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/Users/pradyumna/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/Users/pradyumna/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "Sampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 9802 seconds.\n",
      "/Users/pradyumna/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/Users/pradyumna/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 2 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/!\\ Automatically setting parameter precision...\n"
     ]
    }
   ],
   "source": [
    "# Fitting full GLAM\n",
    "print('Fitting full GLAM hierarchically...')\n",
    "\n",
    "glam_full = glam.GLAM(train_data2)\n",
    "\n",
    "if not os.path.exists(str('results/estimates/glam_PF2019_full_hierarchical_cv'+sufix+'.npy')):\n",
    "    glam_full.make_model('hierarchical', gamma_bounds=(-1, 1), t0_val=0)\n",
    "    glam_full.fit(method='NUTS', tune=1000)\n",
    "else:\n",
    "    print('  Found old parameter estimates in \"results/estimates\". Skipping estimation...')\n",
    "    glam_full.estimates = np.load(str('results/estimates/glam_PF2019_full_hierarchical_cv'+sufix+'.npy'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>p_error</th>\n",
       "      <th>v_mu</th>\n",
       "      <th>v_sd</th>\n",
       "      <th>v</th>\n",
       "      <th>gamma_mu</th>\n",
       "      <th>gamma_sd</th>\n",
       "      <th>gamma</th>\n",
       "      <th>SNR_mu</th>\n",
       "      <th>SNR_sd</th>\n",
       "      <th>SNR</th>\n",
       "      <th>s</th>\n",
       "      <th>tau_mu</th>\n",
       "      <th>tau_sd</th>\n",
       "      <th>tau</th>\n",
       "      <th>t0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>143.48</td>\n",
       "      <td>0.007329</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>146.02</td>\n",
       "      <td>0.005502</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>159.02</td>\n",
       "      <td>0.007660</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>379.60</td>\n",
       "      <td>0.009457</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>162.29</td>\n",
       "      <td>0.006180</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>111.03</td>\n",
       "      <td>0.007508</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>279.71</td>\n",
       "      <td>0.008925</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>157.74</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>64.63</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>143.78</td>\n",
       "      <td>0.009661</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>198.89</td>\n",
       "      <td>0.007469</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>53.71</td>\n",
       "      <td>0.006504</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>69.45</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>303.11</td>\n",
       "      <td>0.011995</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>173.96</td>\n",
       "      <td>0.007718</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>166.75</td>\n",
       "      <td>0.009046</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>353.42</td>\n",
       "      <td>0.007905</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>157.80</td>\n",
       "      <td>0.005119</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>186.99</td>\n",
       "      <td>0.007792</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>167.05</td>\n",
       "      <td>0.009936</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>328.77</td>\n",
       "      <td>0.008353</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>203.80</td>\n",
       "      <td>0.010146</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>193.21</td>\n",
       "      <td>0.006543</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>137.89</td>\n",
       "      <td>0.005769</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>377.20</td>\n",
       "      <td>0.006901</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>185.42</td>\n",
       "      <td>0.006790</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>143.10</td>\n",
       "      <td>0.005148</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>155.42</td>\n",
       "      <td>0.009848</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>50.64</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>107.36</td>\n",
       "      <td>0.008363</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>154.62</td>\n",
       "      <td>0.005619</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>207.82</td>\n",
       "      <td>83.09</td>\n",
       "      <td>176.39</td>\n",
       "      <td>0.007308</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      b  p_error      v_mu      v_sd         v  gamma_mu  gamma_sd  gamma  \\\n",
       "0   1.0     0.05  0.000043  0.000018  0.000048     -0.26      0.17  -0.57   \n",
       "1   1.0     0.05  0.000043  0.000018  0.000017     -0.26      0.17  -0.07   \n",
       "2   1.0     0.05  0.000043  0.000018  0.000039     -0.26      0.17  -0.23   \n",
       "3   1.0     0.05  0.000043  0.000018  0.000027     -0.26      0.17  -0.59   \n",
       "4   1.0     0.05  0.000043  0.000018  0.000032     -0.26      0.17  -0.42   \n",
       "5   1.0     0.05  0.000043  0.000018  0.000067     -0.26      0.17  -0.47   \n",
       "6   1.0     0.05  0.000043  0.000018  0.000028     -0.26      0.17  -0.57   \n",
       "7   1.0     0.05  0.000043  0.000018  0.000054     -0.26      0.17  -0.59   \n",
       "8   1.0     0.05  0.000043  0.000018  0.000102     -0.26      0.17  -0.61   \n",
       "9   1.0     0.05  0.000043  0.000018  0.000066     -0.26      0.17  -0.07   \n",
       "10  1.0     0.05  0.000043  0.000018  0.000029     -0.26      0.17  -0.52   \n",
       "11  1.0     0.05  0.000043  0.000018  0.000072     -0.26      0.17  -0.49   \n",
       "12  1.0     0.05  0.000043  0.000018  0.000034     -0.26      0.17  -0.53   \n",
       "13  1.0     0.05  0.000043  0.000018  0.000039     -0.26      0.17  -0.59   \n",
       "14  1.0     0.05  0.000043  0.000018  0.000045     -0.26      0.17  -0.35   \n",
       "15  1.0     0.05  0.000043  0.000018  0.000045     -0.26      0.17  -0.14   \n",
       "16  1.0     0.05  0.000043  0.000018  0.000021     -0.26      0.17  -0.62   \n",
       "17  1.0     0.05  0.000043  0.000018  0.000030     -0.26      0.17  -0.25   \n",
       "18  1.0     0.05  0.000043  0.000018  0.000041     -0.26      0.17  -0.37   \n",
       "19  1.0     0.05  0.000043  0.000018  0.000040     -0.26      0.17  -0.50   \n",
       "20  1.0     0.05  0.000043  0.000018  0.000036     -0.26      0.17  -0.45   \n",
       "21  1.0     0.05  0.000043  0.000018  0.000035     -0.26      0.17  -0.24   \n",
       "22  1.0     0.05  0.000043  0.000018  0.000034     -0.26      0.17  -0.61   \n",
       "23  1.0     0.05  0.000043  0.000018  0.000042     -0.26      0.17  -0.38   \n",
       "24  1.0     0.05  0.000043  0.000018  0.000018     -0.26      0.17  -0.20   \n",
       "25  1.0     0.05  0.000043  0.000018  0.000035     -0.26      0.17  -0.55   \n",
       "26  1.0     0.05  0.000043  0.000018  0.000024     -0.26      0.17  -0.54   \n",
       "27  1.0     0.05  0.000043  0.000018  0.000063     -0.26      0.17  -0.31   \n",
       "28  1.0     0.05  0.000043  0.000018  0.000098     -0.26      0.17  -0.39   \n",
       "29  1.0     0.05  0.000043  0.000018  0.000074     -0.26      0.17  -0.09   \n",
       "30  1.0     0.05  0.000043  0.000018  0.000045     -0.26      0.17  -0.48   \n",
       "31  1.0     0.05  0.000043  0.000018  0.000045     -0.26      0.17  -0.26   \n",
       "\n",
       "    SNR_mu  SNR_sd     SNR         s  tau_mu  tau_sd   tau   t0  \n",
       "0   207.82   83.09  143.48  0.007329    0.57    0.57  1.00  0.0  \n",
       "1   207.82   83.09  146.02  0.005502    0.57    0.57  1.26  0.0  \n",
       "2   207.82   83.09  159.02  0.007660    0.57    0.57  1.89  0.0  \n",
       "3   207.82   83.09  379.60  0.009457    0.57    0.57  1.69  0.0  \n",
       "4   207.82   83.09  162.29  0.006180    0.57    0.57  2.31  0.0  \n",
       "5   207.82   83.09  111.03  0.007508    0.57    0.57  0.90  0.0  \n",
       "6   207.82   83.09  279.71  0.008925    0.57    0.57  1.91  0.0  \n",
       "7   207.82   83.09  157.74  0.008547    0.57    0.57  0.88  0.0  \n",
       "8   207.82   83.09   64.63  0.006500    0.57    0.57  0.02  0.0  \n",
       "9   207.82   83.09  143.78  0.009661    0.57    0.57  0.25  0.0  \n",
       "10  207.82   83.09  198.89  0.007469    0.57    0.57  1.17  0.0  \n",
       "11  207.82   83.09   53.71  0.006504    0.57    0.57  1.00  0.0  \n",
       "12  207.82   83.09   69.45  0.002445    0.57    0.57  0.22  0.0  \n",
       "13  207.82   83.09  303.11  0.011995    0.57    0.57  0.70  0.0  \n",
       "14  207.82   83.09  173.96  0.007718    0.57    0.57  0.95  0.0  \n",
       "15  207.82   83.09  166.75  0.009046    0.57    0.57  0.36  0.0  \n",
       "16  207.82   83.09  353.42  0.007905    0.57    0.57  0.22  0.0  \n",
       "17  207.82   83.09  157.80  0.005119    0.57    0.57  0.02  0.0  \n",
       "18  207.82   83.09  186.99  0.007792    0.57    0.57  1.25  0.0  \n",
       "19  207.82   83.09  167.05  0.009936    0.57    0.57  0.22  0.0  \n",
       "20  207.82   83.09  328.77  0.008353    0.57    0.57  0.65  0.0  \n",
       "21  207.82   83.09  203.80  0.010146    0.57    0.57  0.25  0.0  \n",
       "22  207.82   83.09  193.21  0.006543    0.57    0.57  1.07  0.0  \n",
       "23  207.82   83.09  137.89  0.005769    0.57    0.57  0.16  0.0  \n",
       "24  207.82   83.09  377.20  0.006901    0.57    0.57  0.74  0.0  \n",
       "25  207.82   83.09  185.42  0.006790    0.57    0.57  0.54  0.0  \n",
       "26  207.82   83.09  143.10  0.005148    0.57    0.57  0.23  0.0  \n",
       "27  207.82   83.09  155.42  0.009848    0.57    0.57  0.41  0.0  \n",
       "28  207.82   83.09   50.64  0.005024    0.57    0.57  0.27  0.0  \n",
       "29  207.82   83.09  107.36  0.008363    0.57    0.57  0.43  0.0  \n",
       "30  207.82   83.09  154.62  0.005619    0.57    0.57  1.19  0.0  \n",
       "31  207.82   83.09  176.39  0.007308    0.57    0.57  0.45  0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save parameter estimates\n",
    "np.save(str('results/estimates/glam_PF2019_nobias_hierarchical_cv'+sufix+'.npy'), glam_full.estimates)\n",
    "pd.DataFrame(glam_full.estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# estimate convergence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rhat parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pradyumna/anaconda3/lib/python3.7/site-packages/arviz/data/io_pymc3.py:89: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n",
      "  FutureWarning,\n",
      "/Users/pradyumna/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gamma</th>\n",
       "      <th>v</th>\n",
       "      <th>tau</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.173949</td>\n",
       "      <td>1.023330</td>\n",
       "      <td>1.280561</td>\n",
       "      <td>1.147912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.898513</td>\n",
       "      <td>2.292834</td>\n",
       "      <td>1.138738</td>\n",
       "      <td>1.831875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.090344</td>\n",
       "      <td>1.265148</td>\n",
       "      <td>1.630727</td>\n",
       "      <td>1.306932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.178904</td>\n",
       "      <td>1.097607</td>\n",
       "      <td>2.107135</td>\n",
       "      <td>1.067656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.212457</td>\n",
       "      <td>1.256147</td>\n",
       "      <td>1.832501</td>\n",
       "      <td>1.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.300630</td>\n",
       "      <td>1.704619</td>\n",
       "      <td>1.690677</td>\n",
       "      <td>1.149749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.498339</td>\n",
       "      <td>1.191384</td>\n",
       "      <td>1.477509</td>\n",
       "      <td>1.051339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.214502</td>\n",
       "      <td>1.595995</td>\n",
       "      <td>1.916048</td>\n",
       "      <td>1.189272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.236163</td>\n",
       "      <td>1.560409</td>\n",
       "      <td>1.498394</td>\n",
       "      <td>1.390899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.182721</td>\n",
       "      <td>1.612793</td>\n",
       "      <td>1.960056</td>\n",
       "      <td>1.191102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.250786</td>\n",
       "      <td>1.306043</td>\n",
       "      <td>1.551166</td>\n",
       "      <td>1.170088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.632055</td>\n",
       "      <td>1.616006</td>\n",
       "      <td>1.397761</td>\n",
       "      <td>1.759665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.035049</td>\n",
       "      <td>1.126678</td>\n",
       "      <td>1.120473</td>\n",
       "      <td>1.069486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.464865</td>\n",
       "      <td>1.419524</td>\n",
       "      <td>2.380123</td>\n",
       "      <td>1.511291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.396694</td>\n",
       "      <td>1.218109</td>\n",
       "      <td>1.309326</td>\n",
       "      <td>1.061500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.964193</td>\n",
       "      <td>1.902962</td>\n",
       "      <td>1.874074</td>\n",
       "      <td>1.288411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.237089</td>\n",
       "      <td>1.918722</td>\n",
       "      <td>1.792011</td>\n",
       "      <td>1.617298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.563925</td>\n",
       "      <td>1.608792</td>\n",
       "      <td>1.946015</td>\n",
       "      <td>1.819774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.752113</td>\n",
       "      <td>2.013502</td>\n",
       "      <td>1.400237</td>\n",
       "      <td>1.619489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.361946</td>\n",
       "      <td>2.049404</td>\n",
       "      <td>1.272030</td>\n",
       "      <td>1.271711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.416530</td>\n",
       "      <td>1.897746</td>\n",
       "      <td>1.843219</td>\n",
       "      <td>1.420045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.936226</td>\n",
       "      <td>1.642819</td>\n",
       "      <td>1.287704</td>\n",
       "      <td>1.059123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.445860</td>\n",
       "      <td>1.527093</td>\n",
       "      <td>1.474590</td>\n",
       "      <td>1.141135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.001449</td>\n",
       "      <td>1.161188</td>\n",
       "      <td>1.121974</td>\n",
       "      <td>1.361106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.181596</td>\n",
       "      <td>1.396146</td>\n",
       "      <td>1.626379</td>\n",
       "      <td>1.338450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.264715</td>\n",
       "      <td>1.355366</td>\n",
       "      <td>1.671728</td>\n",
       "      <td>1.404045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.519348</td>\n",
       "      <td>2.271612</td>\n",
       "      <td>1.773984</td>\n",
       "      <td>1.964432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.547605</td>\n",
       "      <td>1.185415</td>\n",
       "      <td>1.325709</td>\n",
       "      <td>1.339821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.189397</td>\n",
       "      <td>1.481060</td>\n",
       "      <td>1.449514</td>\n",
       "      <td>1.502003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.129724</td>\n",
       "      <td>1.596280</td>\n",
       "      <td>1.454076</td>\n",
       "      <td>1.444147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.213603</td>\n",
       "      <td>2.481296</td>\n",
       "      <td>1.430606</td>\n",
       "      <td>1.694392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2.032869</td>\n",
       "      <td>1.834602</td>\n",
       "      <td>1.475528</td>\n",
       "      <td>1.542612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       gamma         v       tau         s\n",
       "0   1.173949  1.023330  1.280561  1.147912\n",
       "1   1.898513  2.292834  1.138738  1.831875\n",
       "2   1.090344  1.265148  1.630727  1.306932\n",
       "3   1.178904  1.097607  2.107135  1.067656\n",
       "4   1.212457  1.256147  1.832501  1.181818\n",
       "5   1.300630  1.704619  1.690677  1.149749\n",
       "6   1.498339  1.191384  1.477509  1.051339\n",
       "7   1.214502  1.595995  1.916048  1.189272\n",
       "8   1.236163  1.560409  1.498394  1.390899\n",
       "9   1.182721  1.612793  1.960056  1.191102\n",
       "10  1.250786  1.306043  1.551166  1.170088\n",
       "11  1.632055  1.616006  1.397761  1.759665\n",
       "12  2.035049  1.126678  1.120473  1.069486\n",
       "13  1.464865  1.419524  2.380123  1.511291\n",
       "14  1.396694  1.218109  1.309326  1.061500\n",
       "15  1.964193  1.902962  1.874074  1.288411\n",
       "16  2.237089  1.918722  1.792011  1.617298\n",
       "17  1.563925  1.608792  1.946015  1.819774\n",
       "18  1.752113  2.013502  1.400237  1.619489\n",
       "19  1.361946  2.049404  1.272030  1.271711\n",
       "20  1.416530  1.897746  1.843219  1.420045\n",
       "21  1.936226  1.642819  1.287704  1.059123\n",
       "22  1.445860  1.527093  1.474590  1.141135\n",
       "23  2.001449  1.161188  1.121974  1.361106\n",
       "24  1.181596  1.396146  1.626379  1.338450\n",
       "25  1.264715  1.355366  1.671728  1.404045\n",
       "26  1.519348  2.271612  1.773984  1.964432\n",
       "27  1.547605  1.185415  1.325709  1.339821\n",
       "28  1.189397  1.481060  1.449514  1.502003\n",
       "29  1.129724  1.596280  1.454076  1.444147\n",
       "30  1.213603  2.481296  1.430606  1.694392\n",
       "31  2.032869  1.834602  1.475528  1.542612"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trace = glam_full.trace\n",
    "rhats_params = az.rhat(model_trace, method=\"folded\")\n",
    "\n",
    "rhats_params_df = pd.DataFrame()\n",
    "rhats_params_df['gamma'] = rhats_params.gamma.values\n",
    "rhats_params_df['v'] = rhats_params.v.values\n",
    "rhats_params_df['tau'] = rhats_params.tau.values\n",
    "rhats_params_df['s'] = rhats_params.s.values\n",
    "\n",
    "rhats_params_df  # if |rhat - 1 | < 0.05 (rhat: gelman-rubin statistic) the sampler converged "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2. effective sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gamma</th>\n",
       "      <th>v</th>\n",
       "      <th>tau</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.646175</td>\n",
       "      <td>13.229958</td>\n",
       "      <td>6.369122</td>\n",
       "      <td>7.376983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.831094</td>\n",
       "      <td>4.684976</td>\n",
       "      <td>7.316580</td>\n",
       "      <td>5.519295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.794806</td>\n",
       "      <td>5.624054</td>\n",
       "      <td>6.263774</td>\n",
       "      <td>7.054676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.250910</td>\n",
       "      <td>13.142479</td>\n",
       "      <td>5.021127</td>\n",
       "      <td>9.142070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.157859</td>\n",
       "      <td>6.643876</td>\n",
       "      <td>5.381578</td>\n",
       "      <td>9.129132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.481316</td>\n",
       "      <td>5.231344</td>\n",
       "      <td>4.729875</td>\n",
       "      <td>8.897996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.559624</td>\n",
       "      <td>8.000683</td>\n",
       "      <td>5.385089</td>\n",
       "      <td>16.138096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.052468</td>\n",
       "      <td>5.576459</td>\n",
       "      <td>4.683892</td>\n",
       "      <td>9.555966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.260959</td>\n",
       "      <td>5.798601</td>\n",
       "      <td>5.416101</td>\n",
       "      <td>5.441817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.127452</td>\n",
       "      <td>5.860402</td>\n",
       "      <td>5.433941</td>\n",
       "      <td>7.093272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.190373</td>\n",
       "      <td>5.442674</td>\n",
       "      <td>6.690169</td>\n",
       "      <td>6.691395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6.553835</td>\n",
       "      <td>5.272664</td>\n",
       "      <td>5.508749</td>\n",
       "      <td>6.549948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.522957</td>\n",
       "      <td>11.980627</td>\n",
       "      <td>8.930401</td>\n",
       "      <td>8.361503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.407945</td>\n",
       "      <td>7.291691</td>\n",
       "      <td>5.065954</td>\n",
       "      <td>7.070991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.326860</td>\n",
       "      <td>6.711845</td>\n",
       "      <td>6.844274</td>\n",
       "      <td>11.959528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.211526</td>\n",
       "      <td>4.916360</td>\n",
       "      <td>5.111503</td>\n",
       "      <td>7.171300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.422146</td>\n",
       "      <td>5.141586</td>\n",
       "      <td>5.992224</td>\n",
       "      <td>6.503725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.566560</td>\n",
       "      <td>6.233314</td>\n",
       "      <td>4.902795</td>\n",
       "      <td>5.158533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.958687</td>\n",
       "      <td>4.881915</td>\n",
       "      <td>6.098632</td>\n",
       "      <td>5.904062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.754054</td>\n",
       "      <td>4.575122</td>\n",
       "      <td>5.460934</td>\n",
       "      <td>6.423044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.692550</td>\n",
       "      <td>5.219224</td>\n",
       "      <td>5.183874</td>\n",
       "      <td>6.866645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.703113</td>\n",
       "      <td>5.706482</td>\n",
       "      <td>6.129674</td>\n",
       "      <td>9.862346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7.181836</td>\n",
       "      <td>6.921220</td>\n",
       "      <td>6.401261</td>\n",
       "      <td>8.305227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.883386</td>\n",
       "      <td>7.563391</td>\n",
       "      <td>15.893493</td>\n",
       "      <td>7.143833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10.262756</td>\n",
       "      <td>7.125550</td>\n",
       "      <td>5.941296</td>\n",
       "      <td>7.009950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8.339949</td>\n",
       "      <td>5.927148</td>\n",
       "      <td>6.403960</td>\n",
       "      <td>5.956126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12.100979</td>\n",
       "      <td>4.422170</td>\n",
       "      <td>6.088518</td>\n",
       "      <td>4.758628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.182199</td>\n",
       "      <td>7.420908</td>\n",
       "      <td>5.718044</td>\n",
       "      <td>5.856348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>19.095179</td>\n",
       "      <td>5.734376</td>\n",
       "      <td>5.006245</td>\n",
       "      <td>6.524608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.907314</td>\n",
       "      <td>5.977167</td>\n",
       "      <td>6.556678</td>\n",
       "      <td>7.022637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>9.228978</td>\n",
       "      <td>4.702914</td>\n",
       "      <td>5.670407</td>\n",
       "      <td>5.380187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.695716</td>\n",
       "      <td>4.862210</td>\n",
       "      <td>6.303474</td>\n",
       "      <td>5.742666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        gamma          v        tau          s\n",
       "0    8.646175  13.229958   6.369122   7.376983\n",
       "1    5.831094   4.684976   7.316580   5.519295\n",
       "2   12.794806   5.624054   6.263774   7.054676\n",
       "3   12.250910  13.142479   5.021127   9.142070\n",
       "4   11.157859   6.643876   5.381578   9.129132\n",
       "5    7.481316   5.231344   4.729875   8.897996\n",
       "6   10.559624   8.000683   5.385089  16.138096\n",
       "7    7.052468   5.576459   4.683892   9.555966\n",
       "8    7.260959   5.798601   5.416101   5.441817\n",
       "9    9.127452   5.860402   5.433941   7.093272\n",
       "10   7.190373   5.442674   6.690169   6.691395\n",
       "11   6.553835   5.272664   5.508749   6.549948\n",
       "12   5.522957  11.980627   8.930401   8.361503\n",
       "13   6.407945   7.291691   5.065954   7.070991\n",
       "14   6.326860   6.711845   6.844274  11.959528\n",
       "15   5.211526   4.916360   5.111503   7.171300\n",
       "16   6.422146   5.141586   5.992224   6.503725\n",
       "17   8.566560   6.233314   4.902795   5.158533\n",
       "18   4.958687   4.881915   6.098632   5.904062\n",
       "19   5.754054   4.575122   5.460934   6.423044\n",
       "20   5.692550   5.219224   5.183874   6.866645\n",
       "21   9.703113   5.706482   6.129674   9.862346\n",
       "22   7.181836   6.921220   6.401261   8.305227\n",
       "23   4.883386   7.563391  15.893493   7.143833\n",
       "24  10.262756   7.125550   5.941296   7.009950\n",
       "25   8.339949   5.927148   6.403960   5.956126\n",
       "26  12.100979   4.422170   6.088518   4.758628\n",
       "27   5.182199   7.420908   5.718044   5.856348\n",
       "28  19.095179   5.734376   5.006245   6.524608\n",
       "29   5.907314   5.977167   6.556678   7.022637\n",
       "30   9.228978   4.702914   5.670407   5.380187\n",
       "31   4.695716   4.862210   6.303474   5.742666"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ess_model = az.ess(model_trace, relative=False)\n",
    "\n",
    "ess_params_df = pd.DataFrame()\n",
    "ess_params_df['gamma'] = ess_model.gamma.values\n",
    "ess_params_df['v'] = ess_model.v.values\n",
    "ess_params_df['tau'] = ess_model.tau.values\n",
    "ess_params_df['s'] = ess_model.s.values\n",
    "\n",
    "ess_params_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Percentage of divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Divergent 3\n",
      "Percentage of Divergent 0.1\n"
     ]
    }
   ],
   "source": [
    "# display the total number and percentage of divergent\n",
    "divergent = model_trace['diverging']\n",
    "print('Number of Divergent %d' % divergent.nonzero()[0].size)\n",
    "divperc = divergent.nonzero()[0].size / len(model_trace) * 100\n",
    "print('Percentage of Divergent %.1f' % divperc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhats_params_df.to_csv(str('results/convergence/GlamDataPF2019_hierarch_rhatsParams'+sufix+'.csv'))\n",
    "ess_params_df.to_csv(str('results/convergence/GlamDataPF2019_hierarch_essParams'+sufix+'.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waic scores (Less Inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pradyumna/anaconda3/lib/python3.7/site-packages/arviz/stats/stats.py:1415: UserWarning: For one or more samples the posterior variance of the log predictive densities exceeds 0.4. This could be indication of WAIC starting to fail. \n",
      "See http://arxiv.org/abs/1507.04544 for details\n",
      "  \"For one or more samples the posterior variance of the log predictive \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Computed from 8000 by 1 log-likelihood matrix\n",
       "\n",
       "          Estimate       SE\n",
       "elpd_waic -162596.22     0.00\n",
       "p_waic    144992.24        -\n",
       "\n",
       "There has been a warning during the calculation. Please check the results.\n",
       "\n",
       "The scale is now log by default. Use 'scale' argument or 'stats.ic_scale' rcParam if\n",
       "you rely on a specific value.\n",
       "A higher log-score (or a lower deviance) indicates a model with better predictive\n",
       "accuracy."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm.waic(model_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model WAIC 162596.2243666315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pradyumna/anaconda3/lib/python3.7/site-packages/arviz/stats/stats.py:1415: UserWarning: For one or more samples the posterior variance of the log predictive densities exceeds 0.4. This could be indication of WAIC starting to fail. \n",
      "See http://arxiv.org/abs/1507.04544 for details\n",
      "  \"For one or more samples the posterior variance of the log predictive \"\n"
     ]
    }
   ],
   "source": [
    "model_waic = pm.waic(model_trace,scale = 'negative_log')\n",
    "print ('Model WAIC',model_waic.waic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pradyumna/anaconda3/lib/python3.7/site-packages/arviz/stats/stats.py:683: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n",
      "  \"Estimated shape parameter of Pareto distribution is greater than 0.7 for \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Computed from 8000 by 1 log-likelihood matrix\n",
       "\n",
       "          Estimate       SE\n",
       "-elpd_loo 18494.26     0.00\n",
       "p_loo       890.27        -\n",
       "\n",
       "There has been a warning during the calculation. Please check the results.\n",
       "\n",
       "The scale is now log by default. Use 'scale' argument or 'stats.ic_scale' rcParam if\n",
       "you rely on a specific value.\n",
       "A higher log-score (or a lower deviance) indicates a model with better predictive\n",
       "accuracy."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm.loo(model_trace,scale = 'negative_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(str('results/waic/glam_PF2019_full'+ sufix +'.npy'), model_waic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing WAIC scores for full model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "waic() got an unexpected keyword argument 'trace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-bc337b458ea2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Note: DIC computation does not work for ADVI fitted models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# But we are using WAIC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mglam_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_waic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  Found old DIC scores in \"results/waic\". Skipping WAIC computation...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GiTs/glamDDM_PF2019/glam/models.py\u001b[0m in \u001b[0;36mcompute_waic\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_waic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             self.waic = np.array([pm.waic(trace=trace, model=model)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pymc3/stats/__init__.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 )\n\u001b[1;32m     37\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: waic() got an unexpected keyword argument 'trace'"
     ]
    }
   ],
   "source": [
    "# Compute WAICs\n",
    "print('Computing WAIC scores for full model...')\n",
    "if not os.path.exists(str('results/waic/glam_PF2019_full'+ sufix +'.npy')):\n",
    "    # Note: DIC computation does not work for ADVI fitted models\n",
    "    # But we are using WAIC\n",
    "    glam_full.compute_waic()\n",
    "else:\n",
    "    print('  Found old DIC scores in \"results/waic\". Skipping WAIC computation...')\n",
    "    glam_full.waic = np.load(str('results/waic/glam_PF2019_full'+ sufix +'.npy'))\n",
    "\n",
    "# Compute WAICs\n",
    "np.save(str('results/waic/glam_PF2019_full'+ sufix +'.npy'), glam_full.waic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glam_full.waic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute LOO\n",
    "\n",
    "glam_full.loo = pm.loo(trace=glam_full.trace, model=glam_full.model)\n",
    "glam_full.loo\n",
    "np.save(str('results/loo/glam_PF2019_full'+ sufix +'.npy'), glam_full.loo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glam_full.loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "print('Predicting test set data using full GLAM...')\n",
    "glam_full.exchange_data(test_data)\n",
    "\n",
    "if not os.path.exists(str('results/predictions/glam_PF2019_full_hierarchical_cv'+sufix+'.csv')):\n",
    "    glam_full.predict(n_repeats=50)\n",
    "    glam_full.prediction.to_csv(str('results/predictions/glam_PF2019_full_hierarchical_cv'+sufix+'.csv'), index=False)\n",
    "else:\n",
    "    print('  Found old hierarchical full GLAM predictions in \"results/predictions\". Skipping prediction...')\n",
    "    glam_full.prediction = pd.read_csv(str('results/predictions/glam_PF2019_full_hierarchical_cv'+sufix+'.csv'))\n",
    "\n",
    "glam_full.prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. no-bias GLAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting no-bias GLAM\n",
    "print('Fitting no-bias GLAM hierarchically...')\n",
    "\n",
    "glam_nobias = glam.GLAM(train_data)\n",
    "\n",
    "if not os.path.exists(str('results/estimates/glam_PF2019_nobias_hierarchical_cv'+sufix+'.npy')):\n",
    "    glam_nobias.make_model('hierarchical', gamma_val=1.0, t0_val=0)\n",
    "    glam_nobias.fit(method='NUTS', tune=1000)\n",
    "else:\n",
    "    print('  Found old parameter estimates in \"results/estimates\". Skipping estimation...')\n",
    "    glam_nobias.estimates = np.load(str('results/estimates/glam_PF2019_nobias_hierarchical_cv'+sufix+'.npy'))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# Save parameter estimates\n",
    "np.save(str('results/estimates/glam_PF2019_nobias_hierarchical_cv'+sufix+'.npy'), glam_nobias.estimates)\n",
    "pd.DataFrame(glam_nobias.estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case it is already fitted\n",
    "params_part_like = pd.DataFrame.from_dict(glam_nobias.estimates.item(0))\n",
    "params_part_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute LOO\n",
    "\n",
    "glam_nobias.loo = pm.loo(trace=glam_nobias.trace, model=glam_nobias.model)\n",
    "glam_nobias.loo\n",
    "\n",
    "np.save(str('results/loo/glam_PF2019_nobias'+ sufix +'.npy'), glam_nobias.loo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "print('Predicting test set data using no-bias GLAM...')\n",
    "glam_nobias.exchange_data(test_data)\n",
    "\n",
    "if not os.path.exists(str('results/predictions/glam_PF2019_nobias_hierarchical_cv'+sufix+'.csv')):\n",
    "    glam_nobias.predict(n_repeats=50)\n",
    "    glam_nobias.prediction.to_csv(str('results/predictions/glam_PF2019_nobias_hierarchical_cv'+sufix+'.csv'), index=False)\n",
    "else:\n",
    "    print('  Found old hierarchical no-bias GLAM predictions in \"results/predictions\". Skipping prediction...')\n",
    "    glam_nobias.prediction = pd.read_csv(str('results/predictions/glam_PF2019_nobias_hierarchical_cv'+sufix+'.csv'))\n",
    "\n",
    "glam_nobias.prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Plot fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Close Figure to continue...')\n",
    "glam.plot_fit(test_data, [glam_full.prediction]);\n",
    "#glam.plot_fit(test_data, [glam_full.prediction,glam_nobias.prediction]);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for full hierarchical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_participant = glam_full.estimates\n",
    "params_participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_participant = pd.DataFrame.from_dict(glam_full.estimates.item(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Mean gamma \" +  str(params_participant['gamma'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = params_participant[['SNR','gamma','tau','v']].hist(figsize = [20,3] , layout=[1,4],bins = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [END] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
