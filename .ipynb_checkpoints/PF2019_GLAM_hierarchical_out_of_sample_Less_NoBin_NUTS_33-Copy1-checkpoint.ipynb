{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(23) # from random.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1. Hierarchical GLAM estimation and out of sample prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>trial</th>\n",
       "      <th>choice</th>\n",
       "      <th>rt</th>\n",
       "      <th>item_value_0</th>\n",
       "      <th>item_value_1</th>\n",
       "      <th>gaze_0</th>\n",
       "      <th>gaze_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4261.735</td>\n",
       "      <td>110</td>\n",
       "      <td>131</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.396552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3559.258</td>\n",
       "      <td>47</td>\n",
       "      <td>50</td>\n",
       "      <td>0.490772</td>\n",
       "      <td>0.509228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3754.464</td>\n",
       "      <td>50</td>\n",
       "      <td>44</td>\n",
       "      <td>0.490893</td>\n",
       "      <td>0.509107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2431.751</td>\n",
       "      <td>57</td>\n",
       "      <td>50</td>\n",
       "      <td>0.639125</td>\n",
       "      <td>0.360875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2199.342</td>\n",
       "      <td>42</td>\n",
       "      <td>50</td>\n",
       "      <td>0.702232</td>\n",
       "      <td>0.297768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject  trial  choice        rt  item_value_0  item_value_1    gaze_0  \\\n",
       "0        1      0       0  4261.735           110           131  0.603448   \n",
       "1        1      1       1  3559.258            47            50  0.490772   \n",
       "2        1      2       1  3754.464            50            44  0.490893   \n",
       "3        1      3       0  2431.751            57            50  0.639125   \n",
       "4        1      4       0  2199.342            42            50  0.702232   \n",
       "\n",
       "     gaze_1  \n",
       "0  0.396552  \n",
       "1  0.509228  \n",
       "2  0.509107  \n",
       "3  0.360875  \n",
       "4  0.297768  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "sufix = '_hierarchical_Less_NoBin_Gamma-11_NUTS_33'\n",
    "data = pd.read_csv('data/PF2019_data/GlamDataPF2019_Less_NoBin_33.csv')\n",
    "#data = pd.read_csv('data/PF2019_data/GlamDataFF2018_Like_NoBin.csv')\n",
    "\n",
    "# Subset only necessary columns\n",
    "data = data[['subject', 'trial', 'choice', 'rt',\n",
    "         'item_value_0', 'item_value_1',\n",
    "         'gaze_0', 'gaze_1']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data in training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data into training (1920 trials) and test (1920 trials) sets...\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.DataFrame()\n",
    "test_data = pd.DataFrame()\n",
    "\n",
    "for subject in data.subject.unique():\n",
    "    subject_data = data[data['subject'] == subject].copy().reset_index(drop=True)\n",
    "    n_trials = len(subject_data)\n",
    "    \n",
    "    subject_train = subject_data.iloc[np.arange(0, n_trials, 2)].copy()\n",
    "    subject_test = subject_data.iloc[np.arange(1, n_trials, 2)].copy()\n",
    "\n",
    "    test_data = pd.concat([test_data, subject_test])\n",
    "    train_data = pd.concat([train_data, subject_train])\n",
    "\n",
    "test_data.to_csv(str('data/PF2019_data/GlamDataPF2019_preprocessed_test'+sufix+'.csv'))\n",
    "train_data.to_csv(str('data/PF2019_data/GlamDataPF2019_preprocessed_train'+sufix+'.csv'))\n",
    "\n",
    "print('Split data into training ({} trials) and test ({} trials) sets...'.format(len(train_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>trial</th>\n",
       "      <th>choice</th>\n",
       "      <th>rt</th>\n",
       "      <th>item_value_0</th>\n",
       "      <th>item_value_1</th>\n",
       "      <th>gaze_0</th>\n",
       "      <th>gaze_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4261.735</td>\n",
       "      <td>110</td>\n",
       "      <td>131</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.396552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3754.464</td>\n",
       "      <td>50</td>\n",
       "      <td>44</td>\n",
       "      <td>0.490893</td>\n",
       "      <td>0.509107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2199.342</td>\n",
       "      <td>42</td>\n",
       "      <td>50</td>\n",
       "      <td>0.702232</td>\n",
       "      <td>0.297768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6606.698</td>\n",
       "      <td>78</td>\n",
       "      <td>80</td>\n",
       "      <td>0.602831</td>\n",
       "      <td>0.397169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5213.480</td>\n",
       "      <td>50</td>\n",
       "      <td>48</td>\n",
       "      <td>0.475195</td>\n",
       "      <td>0.524805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2007.957</td>\n",
       "      <td>46</td>\n",
       "      <td>50</td>\n",
       "      <td>0.638397</td>\n",
       "      <td>0.361603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1978.076</td>\n",
       "      <td>50</td>\n",
       "      <td>56</td>\n",
       "      <td>0.605235</td>\n",
       "      <td>0.394765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>3907.246</td>\n",
       "      <td>110</td>\n",
       "      <td>124</td>\n",
       "      <td>0.479771</td>\n",
       "      <td>0.520229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1823.377</td>\n",
       "      <td>64</td>\n",
       "      <td>80</td>\n",
       "      <td>0.629489</td>\n",
       "      <td>0.370511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>4920.474</td>\n",
       "      <td>101</td>\n",
       "      <td>110</td>\n",
       "      <td>0.694959</td>\n",
       "      <td>0.305041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1705.811</td>\n",
       "      <td>50</td>\n",
       "      <td>42</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.363564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2474.714</td>\n",
       "      <td>77</td>\n",
       "      <td>80</td>\n",
       "      <td>0.642971</td>\n",
       "      <td>0.357029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>3790.480</td>\n",
       "      <td>44</td>\n",
       "      <td>50</td>\n",
       "      <td>0.565495</td>\n",
       "      <td>0.434505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>3822.251</td>\n",
       "      <td>50</td>\n",
       "      <td>46</td>\n",
       "      <td>0.468667</td>\n",
       "      <td>0.531333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>2252.245</td>\n",
       "      <td>133</td>\n",
       "      <td>110</td>\n",
       "      <td>0.519646</td>\n",
       "      <td>0.480354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>4155.502</td>\n",
       "      <td>126</td>\n",
       "      <td>110</td>\n",
       "      <td>0.670523</td>\n",
       "      <td>0.329477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>5814.551</td>\n",
       "      <td>110</td>\n",
       "      <td>126</td>\n",
       "      <td>0.763474</td>\n",
       "      <td>0.236526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>1743.991</td>\n",
       "      <td>80</td>\n",
       "      <td>77</td>\n",
       "      <td>0.657066</td>\n",
       "      <td>0.342934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>3381.750</td>\n",
       "      <td>80</td>\n",
       "      <td>64</td>\n",
       "      <td>0.627425</td>\n",
       "      <td>0.372575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>3602.313</td>\n",
       "      <td>124</td>\n",
       "      <td>110</td>\n",
       "      <td>0.470957</td>\n",
       "      <td>0.529043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>7964.301</td>\n",
       "      <td>128</td>\n",
       "      <td>110</td>\n",
       "      <td>0.539804</td>\n",
       "      <td>0.460196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>2122.334</td>\n",
       "      <td>54</td>\n",
       "      <td>50</td>\n",
       "      <td>0.699431</td>\n",
       "      <td>0.300569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>2201.475</td>\n",
       "      <td>105</td>\n",
       "      <td>110</td>\n",
       "      <td>0.585813</td>\n",
       "      <td>0.414187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>2017.429</td>\n",
       "      <td>96</td>\n",
       "      <td>80</td>\n",
       "      <td>0.671605</td>\n",
       "      <td>0.328395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>2528.386</td>\n",
       "      <td>110</td>\n",
       "      <td>128</td>\n",
       "      <td>0.717070</td>\n",
       "      <td>0.282930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1513.221</td>\n",
       "      <td>110</td>\n",
       "      <td>119</td>\n",
       "      <td>0.638326</td>\n",
       "      <td>0.361674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>3815.203</td>\n",
       "      <td>58</td>\n",
       "      <td>50</td>\n",
       "      <td>0.563237</td>\n",
       "      <td>0.436763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>3013.624</td>\n",
       "      <td>74</td>\n",
       "      <td>80</td>\n",
       "      <td>0.474341</td>\n",
       "      <td>0.525659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>1818.684</td>\n",
       "      <td>66</td>\n",
       "      <td>80</td>\n",
       "      <td>0.665297</td>\n",
       "      <td>0.334703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>3156.807</td>\n",
       "      <td>94</td>\n",
       "      <td>110</td>\n",
       "      <td>0.764561</td>\n",
       "      <td>0.235439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>33</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>5574.181</td>\n",
       "      <td>75</td>\n",
       "      <td>80</td>\n",
       "      <td>0.473662</td>\n",
       "      <td>0.526338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>33</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>1623.716</td>\n",
       "      <td>89</td>\n",
       "      <td>110</td>\n",
       "      <td>0.581980</td>\n",
       "      <td>0.418020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>33</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>5266.289</td>\n",
       "      <td>110</td>\n",
       "      <td>98</td>\n",
       "      <td>0.485209</td>\n",
       "      <td>0.514791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>33</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>4480.192</td>\n",
       "      <td>110</td>\n",
       "      <td>94</td>\n",
       "      <td>0.480105</td>\n",
       "      <td>0.519895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>33</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>2559.781</td>\n",
       "      <td>50</td>\n",
       "      <td>54</td>\n",
       "      <td>0.416775</td>\n",
       "      <td>0.583225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>33</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>4939.104</td>\n",
       "      <td>50</td>\n",
       "      <td>52</td>\n",
       "      <td>0.480362</td>\n",
       "      <td>0.519638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>33</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>3524.365</td>\n",
       "      <td>87</td>\n",
       "      <td>110</td>\n",
       "      <td>0.515382</td>\n",
       "      <td>0.484618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>33</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>4286.780</td>\n",
       "      <td>80</td>\n",
       "      <td>96</td>\n",
       "      <td>0.495837</td>\n",
       "      <td>0.504163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>33</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>4953.792</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>0.505023</td>\n",
       "      <td>0.494977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>33</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>1525.091</td>\n",
       "      <td>45</td>\n",
       "      <td>50</td>\n",
       "      <td>0.535368</td>\n",
       "      <td>0.464632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>33</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>5318.646</td>\n",
       "      <td>50</td>\n",
       "      <td>53</td>\n",
       "      <td>0.571120</td>\n",
       "      <td>0.428880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>33</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>1867.922</td>\n",
       "      <td>80</td>\n",
       "      <td>88</td>\n",
       "      <td>0.612229</td>\n",
       "      <td>0.387771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>33</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>4005.469</td>\n",
       "      <td>103</td>\n",
       "      <td>110</td>\n",
       "      <td>0.416217</td>\n",
       "      <td>0.583783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>33</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>4756.104</td>\n",
       "      <td>80</td>\n",
       "      <td>72</td>\n",
       "      <td>0.413177</td>\n",
       "      <td>0.586823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>33</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>2002.718</td>\n",
       "      <td>80</td>\n",
       "      <td>90</td>\n",
       "      <td>0.633215</td>\n",
       "      <td>0.366785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>33</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>2154.276</td>\n",
       "      <td>80</td>\n",
       "      <td>70</td>\n",
       "      <td>0.499230</td>\n",
       "      <td>0.500770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>33</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>3600.589</td>\n",
       "      <td>82</td>\n",
       "      <td>80</td>\n",
       "      <td>0.500618</td>\n",
       "      <td>0.499382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>33</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>3099.150</td>\n",
       "      <td>96</td>\n",
       "      <td>110</td>\n",
       "      <td>0.469571</td>\n",
       "      <td>0.530429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>33</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>4201.412</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>0.466509</td>\n",
       "      <td>0.533491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>33</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>2128.968</td>\n",
       "      <td>110</td>\n",
       "      <td>96</td>\n",
       "      <td>0.465549</td>\n",
       "      <td>0.534451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>33</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>2593.981</td>\n",
       "      <td>49</td>\n",
       "      <td>50</td>\n",
       "      <td>0.572452</td>\n",
       "      <td>0.427548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>33</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>3422.548</td>\n",
       "      <td>51</td>\n",
       "      <td>50</td>\n",
       "      <td>0.421237</td>\n",
       "      <td>0.578763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>33</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>3026.628</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>0.599172</td>\n",
       "      <td>0.400828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>33</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>4261.328</td>\n",
       "      <td>85</td>\n",
       "      <td>80</td>\n",
       "      <td>0.426795</td>\n",
       "      <td>0.573205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>33</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>2831.685</td>\n",
       "      <td>50</td>\n",
       "      <td>43</td>\n",
       "      <td>0.406985</td>\n",
       "      <td>0.593015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>33</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>1522.433</td>\n",
       "      <td>50</td>\n",
       "      <td>60</td>\n",
       "      <td>0.500810</td>\n",
       "      <td>0.499190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>33</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>2271.982</td>\n",
       "      <td>122</td>\n",
       "      <td>110</td>\n",
       "      <td>0.500242</td>\n",
       "      <td>0.499758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>33</td>\n",
       "      <td>114</td>\n",
       "      <td>1</td>\n",
       "      <td>3589.465</td>\n",
       "      <td>80</td>\n",
       "      <td>67</td>\n",
       "      <td>0.494589</td>\n",
       "      <td>0.505411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>33</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>6127.148</td>\n",
       "      <td>110</td>\n",
       "      <td>122</td>\n",
       "      <td>0.454173</td>\n",
       "      <td>0.545827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>33</td>\n",
       "      <td>118</td>\n",
       "      <td>1</td>\n",
       "      <td>3218.693</td>\n",
       "      <td>110</td>\n",
       "      <td>112</td>\n",
       "      <td>0.475219</td>\n",
       "      <td>0.524781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1920 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject  trial  choice        rt  item_value_0  item_value_1    gaze_0  \\\n",
       "0          1      0       0  4261.735           110           131  0.603448   \n",
       "2          1      2       1  3754.464            50            44  0.490893   \n",
       "4          1      4       0  2199.342            42            50  0.702232   \n",
       "6          1      6       0  6606.698            78            80  0.602831   \n",
       "8          1      8       1  5213.480            50            48  0.475195   \n",
       "10         1     10       0  2007.957            46            50  0.638397   \n",
       "12         1     12       0  1978.076            50            56  0.605235   \n",
       "14         1     14       1  3907.246           110           124  0.479771   \n",
       "16         1     16       0  1823.377            64            80  0.629489   \n",
       "18         1     18       0  4920.474           101           110  0.694959   \n",
       "20         1     20       0  1705.811            50            42  0.636436   \n",
       "22         1     22       0  2474.714            77            80  0.642971   \n",
       "24         1     24       0  3790.480            44            50  0.565495   \n",
       "26         1     26       1  3822.251            50            46  0.468667   \n",
       "28         1     28       1  2252.245           133           110  0.519646   \n",
       "30         1     30       1  4155.502           126           110  0.670523   \n",
       "32         1     32       0  5814.551           110           126  0.763474   \n",
       "34         1     34       0  1743.991            80            77  0.657066   \n",
       "36         1     36       1  3381.750            80            64  0.627425   \n",
       "38         1     38       1  3602.313           124           110  0.470957   \n",
       "40         1     40       1  7964.301           128           110  0.539804   \n",
       "42         1     42       0  2122.334            54            50  0.699431   \n",
       "44         1     44       0  2201.475           105           110  0.585813   \n",
       "46         1     46       0  2017.429            96            80  0.671605   \n",
       "48         1     48       0  2528.386           110           128  0.717070   \n",
       "50         1     50       0  1513.221           110           119  0.638326   \n",
       "52         1     52       0  3815.203            58            50  0.563237   \n",
       "54         1     54       0  3013.624            74            80  0.474341   \n",
       "56         1     56       0  1818.684            66            80  0.665297   \n",
       "58         1     58       0  3156.807            94           110  0.764561   \n",
       "..       ...    ...     ...       ...           ...           ...       ...   \n",
       "60        33     60       0  5574.181            75            80  0.473662   \n",
       "62        33     62       0  1623.716            89           110  0.581980   \n",
       "64        33     64       1  5266.289           110            98  0.485209   \n",
       "66        33     66       1  4480.192           110            94  0.480105   \n",
       "68        33     68       1  2559.781            50            54  0.416775   \n",
       "70        33     70       1  4939.104            50            52  0.480362   \n",
       "72        33     72       0  3524.365            87           110  0.515382   \n",
       "74        33     74       0  4286.780            80            96  0.495837   \n",
       "76        33     76       0  4953.792            50            55  0.505023   \n",
       "78        33     78       0  1525.091            45            50  0.535368   \n",
       "80        33     80       1  5318.646            50            53  0.571120   \n",
       "82        33     82       0  1867.922            80            88  0.612229   \n",
       "84        33     84       0  4005.469           103           110  0.416217   \n",
       "86        33     86       0  4756.104            80            72  0.413177   \n",
       "88        33     88       0  2002.718            80            90  0.633215   \n",
       "90        33     90       1  2154.276            80            70  0.499230   \n",
       "92        33     92       1  3600.589            82            80  0.500618   \n",
       "94        33     94       0  3099.150            96           110  0.469571   \n",
       "96        33     96       1  4201.412            60            50  0.466509   \n",
       "98        33     98       1  2128.968           110            96  0.465549   \n",
       "100       33    100       0  2593.981            49            50  0.572452   \n",
       "102       33    102       1  3422.548            51            50  0.421237   \n",
       "104       33    104       0  3026.628            40            50  0.599172   \n",
       "106       33    106       1  4261.328            85            80  0.426795   \n",
       "108       33    108       1  2831.685            50            43  0.406985   \n",
       "110       33    110       0  1522.433            50            60  0.500810   \n",
       "112       33    112       1  2271.982           122           110  0.500242   \n",
       "114       33    114       1  3589.465            80            67  0.494589   \n",
       "116       33    116       0  6127.148           110           122  0.454173   \n",
       "118       33    118       1  3218.693           110           112  0.475219   \n",
       "\n",
       "       gaze_1  \n",
       "0    0.396552  \n",
       "2    0.509107  \n",
       "4    0.297768  \n",
       "6    0.397169  \n",
       "8    0.524805  \n",
       "10   0.361603  \n",
       "12   0.394765  \n",
       "14   0.520229  \n",
       "16   0.370511  \n",
       "18   0.305041  \n",
       "20   0.363564  \n",
       "22   0.357029  \n",
       "24   0.434505  \n",
       "26   0.531333  \n",
       "28   0.480354  \n",
       "30   0.329477  \n",
       "32   0.236526  \n",
       "34   0.342934  \n",
       "36   0.372575  \n",
       "38   0.529043  \n",
       "40   0.460196  \n",
       "42   0.300569  \n",
       "44   0.414187  \n",
       "46   0.328395  \n",
       "48   0.282930  \n",
       "50   0.361674  \n",
       "52   0.436763  \n",
       "54   0.525659  \n",
       "56   0.334703  \n",
       "58   0.235439  \n",
       "..        ...  \n",
       "60   0.526338  \n",
       "62   0.418020  \n",
       "64   0.514791  \n",
       "66   0.519895  \n",
       "68   0.583225  \n",
       "70   0.519638  \n",
       "72   0.484618  \n",
       "74   0.504163  \n",
       "76   0.494977  \n",
       "78   0.464632  \n",
       "80   0.428880  \n",
       "82   0.387771  \n",
       "84   0.583783  \n",
       "86   0.586823  \n",
       "88   0.366785  \n",
       "90   0.500770  \n",
       "92   0.499382  \n",
       "94   0.530429  \n",
       "96   0.533491  \n",
       "98   0.534451  \n",
       "100  0.427548  \n",
       "102  0.578763  \n",
       "104  0.400828  \n",
       "106  0.573205  \n",
       "108  0.593015  \n",
       "110  0.499190  \n",
       "112  0.499758  \n",
       "114  0.505411  \n",
       "116  0.545827  \n",
       "118  0.524781  \n",
       "\n",
       "[1920 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical GLAM estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. full GLAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting full GLAM hierarchically...\n",
      "Generating single subject models for 32 subjects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 32 model(s) using NUTS...\n",
      "  Fitting model 1 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:16<00:00, 741.36draws/s]\n",
      "There were 117 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.4384230582815024, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "There were 18 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 5 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 17 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 2 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:13<00:00, 860.25draws/s]\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 3 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:17<00:00, 688.25draws/s]\n",
      "There were 322 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 406 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 358 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 309 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 4 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:13<00:00, 910.49draws/s]\n",
      "There were 608 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 478 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 446 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 491 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 5 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:13<00:00, 864.05draws/s]\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 6 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [03:47<00:00, 24.55draws/s]\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 7 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:11<00:00, 1010.43draws/s]\n",
      "There were 12 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 3 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 16 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 30 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.9260408310706477, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 8 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:10<00:00, 1104.22draws/s]\n",
      "The acceptance probability does not match the target. It is 0.6373643268837982, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.8822116179896194, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 9 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [12:54<00:00,  5.12draws/s]\n",
      "The acceptance probability does not match the target. It is 0.6610998779293699, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.6088156364577227, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The gelman-rubin statistic is larger than 1.4 for some parameters. The sampler did not converge.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 10 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:11<00:00, 1034.62draws/s]\n",
      "There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 5 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 7 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 11 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:14<00:00, 821.95draws/s]\n",
      "There were 167 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 90 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 137 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 365 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.4006640993008891, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 12 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:15<00:00, 756.59draws/s]\n",
      "There were 31 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 30 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 28 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 34 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 13 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [15:55<00:00,  4.08draws/s]\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The gelman-rubin statistic is larger than 1.4 for some parameters. The sampler did not converge.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 14 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:10<00:00, 1108.49draws/s]\n",
      "There were 549 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 472 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 595 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 520 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 15 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:13<00:00, 888.94draws/s]\n",
      "There were 2 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 16 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:12<00:00, 949.17draws/s]\n",
      "There were 229 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 117 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.8998564411294767, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "There were 218 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 157 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.8870800567641581, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 17 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:12<00:00, 926.45draws/s]\n",
      "There were 232 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 244 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 234 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 207 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.8901675102707732, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 18 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:09<00:00, 1222.91draws/s]\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 19 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:14<00:00, 849.76draws/s]\n",
      "There were 255 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 253 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 277 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 269 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 20 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:09<00:00, 1319.48draws/s]\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 21 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:08<00:00, 1384.46draws/s]\n",
      "There were 2 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n",
      "There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 22 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:11<00:00, 1044.51draws/s]\n",
      "There were 136 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 122 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.671986043327847, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "There were 131 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 164 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 23 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:08<00:00, 1367.07draws/s]\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 24 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:09<00:00, 1274.20draws/s]\n",
      "There were 27 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 48 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.6171022991906691, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "There were 37 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.8812632740248372, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "There were 12 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 25 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:09<00:00, 1301.44draws/s]\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 26 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:08<00:00, 1361.78draws/s]\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 27 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:08<00:00, 1443.42draws/s]\n",
      "There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 28 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:09<00:00, 1327.75draws/s]\n",
      "There were 283 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 388 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 300 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 277 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 29 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [11:38<00:00,  4.68draws/s]\n",
      "The acceptance probability does not match the target. It is 0.9222877664701582, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The gelman-rubin statistic is larger than 1.05 for some parameters. This indicates slight problems during sampling.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 30 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:09<00:00, 1246.27draws/s]\n",
      "There were 88 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 31 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 70 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 30 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.8791096390062014, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 31 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:10<00:00, 1112.43draws/s]\n",
      "The number of effective samples is smaller than 25% for some parameters.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting model 32 of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [tau, SNR, gamma, v]\n",
      "Sampling 4 chains: 100%|██████████| 12000/12000 [00:09<00:00, 1224.34draws/s]\n",
      "There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n",
      "There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 25% for some parameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/!\\ Automatically setting parameter precision...\n"
     ]
    }
   ],
   "source": [
    "# Fitting full GLAM\n",
    "print('Fitting full GLAM hierarchically...')\n",
    "\n",
    "glam_full = glam.GLAM(train_data)\n",
    "\n",
    "if not os.path.exists(str('results/estimates/glam_PF2019_full_hierarchical_cv'+sufix+'.npy')):\n",
    "    glam_full.make_model('individual', gamma_bounds=(-1, 1), t0_val=0)\n",
    "    glam_full.fit(method='NUTS', tune=1000)\n",
    "else:\n",
    "    print('  Found old parameter estimates in \"results/estimates\". Skipping estimation...')\n",
    "    glam_full.estimates = np.load(str('results/estimates/glam_PF2019_full_hierarchical_cv'+sufix+'.npy'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNR</th>\n",
       "      <th>b</th>\n",
       "      <th>gamma</th>\n",
       "      <th>p_error</th>\n",
       "      <th>s</th>\n",
       "      <th>t0</th>\n",
       "      <th>tau</th>\n",
       "      <th>v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>133.85</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007718</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>322.12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006114</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>195.74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006339</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.000034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>435.35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008508</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>169.32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.005410</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>107.39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.005416</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>219.15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007774</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>129.52</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006626</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.000051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>48.38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.005459</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>160.39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.009785</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>239.45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007605</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>56.19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.000082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>85.09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.002561</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>251.57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.009641</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>138.64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006479</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>157.31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008105</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>486.24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008294</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>168.95</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>158.43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008768</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>169.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008778</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>191.79</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006710</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>171.92</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.010861</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>238.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007732</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>146.31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006475</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>329.45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006911</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>168.99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006710</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>130.61</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.004379</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>201.86</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.009143</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>60.19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>130.35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.008675</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>166.76</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006280</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>154.12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.007231</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SNR    b  gamma  p_error         s     t0   tau         v\n",
       "0   133.85  1.0  -0.92     0.05  0.007718  [0.0]  0.01  0.000052\n",
       "1   322.12  1.0  -0.99     0.05  0.006114  [0.0]  0.05  0.000016\n",
       "2   195.74  1.0  -0.10     0.05  0.006339  [0.0]  0.14  0.000034\n",
       "3   435.35  1.0  -0.99     0.05  0.008508  [0.0]  0.11  0.000021\n",
       "4   169.32  1.0  -0.99     0.05  0.005410  [0.0]  0.06  0.000031\n",
       "5   107.39  1.0  -0.70     0.05  0.005416  [0.0]  0.08  0.000058\n",
       "6   219.15  1.0  -0.99     0.05  0.007774  [0.0]  0.09  0.000030\n",
       "7   129.52  1.0  -0.99     0.05  0.006626  [0.0]  0.09  0.000051\n",
       "8    48.38  1.0  -0.84     0.05  0.005459  [0.0]  0.00  0.000114\n",
       "9   160.39  1.0  -0.99     0.05  0.009785  [0.0]  0.06  0.000055\n",
       "10  239.45  1.0  -0.91     0.05  0.007605  [0.0]  0.03  0.000026\n",
       "11   56.19  1.0  -0.96     0.05  0.005168  [0.0]  0.02  0.000082\n",
       "12   85.09  1.0   0.95     0.05  0.002561  [0.0]  0.00  0.000033\n",
       "13  251.57  1.0  -0.81     0.05  0.009641  [0.0]  0.26  0.000042\n",
       "14  138.64  1.0  -0.96     0.05  0.006479  [0.0]  0.04  0.000045\n",
       "15  157.31  1.0  -0.98     0.05  0.008105  [0.0]  0.08  0.000045\n",
       "16  486.24  1.0  -0.99     0.05  0.008294  [0.0]  0.03  0.000018\n",
       "17  168.95  1.0  -0.98     0.05  0.005192  [0.0]  0.05  0.000028\n",
       "18  158.43  1.0  -0.88     0.05  0.008768  [0.0]  0.04  0.000032\n",
       "19  169.82  1.0  -0.97     0.05  0.008778  [0.0]  0.03  0.000045\n",
       "20  191.79  1.0  -0.98     0.05  0.006710  [0.0]  0.09  0.000032\n",
       "21  171.92  1.0  -0.98     0.05  0.010861  [0.0]  0.06  0.000035\n",
       "22  238.20  1.0  -0.99     0.05  0.007732  [0.0]  0.07  0.000030\n",
       "23  146.31  1.0  -0.99     0.05  0.006475  [0.0]  0.04  0.000037\n",
       "24  329.45  1.0  -0.99     0.05  0.006911  [0.0]  0.02  0.000017\n",
       "25  168.99  1.0  -0.58     0.05  0.006710  [0.0]  0.06  0.000039\n",
       "26  130.61  1.0  -0.99     0.05  0.004379  [0.0]  0.03  0.000029\n",
       "27  201.86  1.0  -0.69     0.05  0.009143  [0.0]  0.15  0.000047\n",
       "28   60.19  1.0  -0.99     0.05  0.003964  [0.0]  0.12  0.000069\n",
       "29  130.35  1.0  -0.99     0.05  0.008675  [0.0]  0.06  0.000057\n",
       "30  166.76  1.0  -0.99     0.05  0.006280  [0.0]  0.03  0.000042\n",
       "31  154.12  1.0  -0.99     0.05  0.007231  [0.0]  0.05  0.000042"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save parameter estimates\n",
    "np.save(str('results/estimates/glam_PF2019_nobias_hierarchical_cv'+sufix+'.npy'), glam_full.estimates)\n",
    "pd.DataFrame(glam_full.estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing WAIC scores for full model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.7/site-packages/pymc3/stats.py:211: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "# Compute WAICs\n",
    "print('Computing WAIC scores for full model...')\n",
    "if not os.path.exists(str('results/waic/glam_PF2019_full'+ sufix +'.npy')):\n",
    "    # Note: DIC computation does not work for ADVI fitted models\n",
    "    # But we are using WAIC\n",
    "    glam_full.compute_waic()\n",
    "else:\n",
    "    print('  Found old DIC scores in \"results/waic\". Skipping WAIC computation...')\n",
    "    glam_full.waic = np.load(str('results/waic/glam_PF2019_full'+ sufix +'.npy'))\n",
    "\n",
    "# Compute WAICs\n",
    "np.save(str('results/waic/glam_PF2019_full'+ sufix +'.npy'), glam_full.waic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.08852846e+03, 0.00000000e+00, 3.39019362e+00, 1.00000000e+00],\n",
       "       [1.20716973e+03, 0.00000000e+00, 2.25159602e+00, 1.00000000e+00],\n",
       "       [1.02923746e+03, 0.00000000e+00, 3.50081020e+00, 1.00000000e+00],\n",
       "       [1.21420795e+03, 0.00000000e+00, 2.98417075e+00, 1.00000000e+00],\n",
       "       [1.11910393e+03, 0.00000000e+00, 2.91006834e+00, 1.00000000e+00],\n",
       "       [1.00633302e+03, 0.00000000e+00, 2.15822102e+00, 1.00000000e+00],\n",
       "       [1.14927457e+03, 0.00000000e+00, 2.90198273e+00, 1.00000000e+00],\n",
       "       [1.03287199e+03, 0.00000000e+00, 2.55902722e+00, 1.00000000e+00],\n",
       "       [9.72317555e+02, 0.00000000e+00, 1.71952511e+00, 1.00000000e+00],\n",
       "       [1.06370472e+03, 0.00000000e+00, 3.80614524e+00, 1.00000000e+00],\n",
       "       [1.15588170e+03, 0.00000000e+00, 3.91394220e+00, 1.00000000e+00],\n",
       "       [1.00350065e+03, 0.00000000e+00, 3.20773600e+00, 1.00000000e+00],\n",
       "       [1.05704204e+03, 0.00000000e+00, 7.74185396e+00, 1.00000000e+00],\n",
       "       [1.07738791e+03, 0.00000000e+00, 2.43946549e+00, 1.00000000e+00],\n",
       "       [1.05073670e+03, 0.00000000e+00, 2.74116374e+00, 1.00000000e+00],\n",
       "       [1.06390572e+03, 0.00000000e+00, 4.97190172e+00, 1.00000000e+00],\n",
       "       [1.22092103e+03, 0.00000000e+00, 2.29112870e+00, 1.00000000e+00],\n",
       "       [1.11580464e+03, 0.00000000e+00, 3.40243963e+00, 1.00000000e+00],\n",
       "       [1.09798282e+03, 0.00000000e+00, 3.17372010e+00, 1.00000000e+00],\n",
       "       [1.10258922e+03, 0.00000000e+00, 2.47990988e+00, 1.00000000e+00],\n",
       "       [1.10136455e+03, 0.00000000e+00, 2.80468483e+00, 1.00000000e+00],\n",
       "       [1.11974516e+03, 0.00000000e+00, 3.14407796e+00, 1.00000000e+00],\n",
       "       [1.13433423e+03, 0.00000000e+00, 2.93014660e+00, 1.00000000e+00],\n",
       "       [1.10909440e+03, 0.00000000e+00, 3.94641503e+00, 1.00000000e+00],\n",
       "       [1.24124661e+03, 0.00000000e+00, 1.70560586e+00, 1.00000000e+00],\n",
       "       [1.08566357e+03, 0.00000000e+00, 2.25130120e+00, 1.00000000e+00],\n",
       "       [1.11025928e+03, 0.00000000e+00, 2.78007623e+00, 1.00000000e+00],\n",
       "       [1.04479566e+03, 0.00000000e+00, 3.46086227e+00, 1.00000000e+00],\n",
       "       [8.84485700e+02, 0.00000000e+00, 2.46633947e+00, 1.00000000e+00],\n",
       "       [1.02028526e+03, 0.00000000e+00, 3.59830072e+00, 1.00000000e+00],\n",
       "       [1.05107478e+03, 0.00000000e+00, 1.85314252e+00, 1.00000000e+00],\n",
       "       [1.10278694e+03, 0.00000000e+00, 1.95866311e+00, 1.00000000e+00],\n",
       "       [1.11493280e+03, 0.00000000e+00, 1.95635618e+00, 1.00000000e+00]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glam_full.waic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'nchains'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f3d14e55e290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Compute LOO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mglam_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglam_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglam_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mglam_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results/loo/glam_PF2019_full'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0msufix\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglam_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pymc3/stats.py\u001b[0m in \u001b[0;36mloo\u001b[0;34m(trace, model, pointwise, reff, progressbar)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreff\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnchains\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m             \u001b[0mreff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'nchains'"
     ]
    }
   ],
   "source": [
    "# Compute LOO\n",
    "\n",
    "glam_full.loo = pm.loo(trace=glam_full.trace, model=glam_full.model)\n",
    "glam_full.loo\n",
    "np.save(str('results/loo/glam_PF2019_full'+ sufix +'.npy'), glam_full.loo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GLAM' object has no attribute 'loo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-12528b9778fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mglam_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'GLAM' object has no attribute 'loo'"
     ]
    }
   ],
   "source": [
    "glam_full.loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting test set data using full GLAM...\n",
      "Replaced attached data (1980 trials) with new data (1980 trials)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>trial</th>\n",
       "      <th>repeat</th>\n",
       "      <th>choice</th>\n",
       "      <th>rt</th>\n",
       "      <th>item_value_0</th>\n",
       "      <th>gaze_0</th>\n",
       "      <th>item_value_1</th>\n",
       "      <th>gaze_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4618.0</td>\n",
       "      <td>47</td>\n",
       "      <td>0.490772</td>\n",
       "      <td>50</td>\n",
       "      <td>0.509228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3511.0</td>\n",
       "      <td>47</td>\n",
       "      <td>0.490772</td>\n",
       "      <td>50</td>\n",
       "      <td>0.509228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3231.0</td>\n",
       "      <td>47</td>\n",
       "      <td>0.490772</td>\n",
       "      <td>50</td>\n",
       "      <td>0.509228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4558.0</td>\n",
       "      <td>47</td>\n",
       "      <td>0.490772</td>\n",
       "      <td>50</td>\n",
       "      <td>0.509228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2279.0</td>\n",
       "      <td>47</td>\n",
       "      <td>0.490772</td>\n",
       "      <td>50</td>\n",
       "      <td>0.509228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject  trial  repeat  choice      rt  item_value_0    gaze_0  \\\n",
       "0      0.0    0.0     0.0     1.0  4618.0            47  0.490772   \n",
       "1      0.0    0.0     1.0     0.0  3511.0            47  0.490772   \n",
       "2      0.0    0.0     2.0     1.0  3231.0            47  0.490772   \n",
       "3      0.0    0.0     3.0     1.0  4558.0            47  0.490772   \n",
       "4      0.0    0.0     4.0     1.0  2279.0            47  0.490772   \n",
       "\n",
       "   item_value_1    gaze_1  \n",
       "0            50  0.509228  \n",
       "1            50  0.509228  \n",
       "2            50  0.509228  \n",
       "3            50  0.509228  \n",
       "4            50  0.509228  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions\n",
    "print('Predicting test set data using full GLAM...')\n",
    "glam_full.exchange_data(test_data)\n",
    "\n",
    "if not os.path.exists(str('results/predictions/glam_PF2019_full_hierarchical_cv'+sufix+'.csv')):\n",
    "    glam_full.predict(n_repeats=50)\n",
    "    glam_full.prediction.to_csv(str('results/predictions/glam_PF2019_full_hierarchical_cv'+sufix+'.csv'), index=False)\n",
    "else:\n",
    "    print('  Found old hierarchical full GLAM predictions in \"results/predictions\". Skipping prediction...')\n",
    "    glam_full.prediction = pd.read_csv(str('results/predictions/glam_PF2019_full_hierarchical_cv'+sufix+'.csv'))\n",
    "\n",
    "glam_full.prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. no-bias GLAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting no-bias GLAM\n",
    "print('Fitting no-bias GLAM hierarchically...')\n",
    "\n",
    "glam_nobias = glam.GLAM(train_data)\n",
    "\n",
    "if not os.path.exists(str('results/estimates/glam_PF2019_nobias_hierarchical_cv'+sufix+'.npy')):\n",
    "    glam_nobias.make_model('hierarchical', gamma_val=1.0, t0_val=0)\n",
    "    glam_nobias.fit(method='NUTS', tune=1000)\n",
    "else:\n",
    "    print('  Found old parameter estimates in \"results/estimates\". Skipping estimation...')\n",
    "    glam_nobias.estimates = np.load(str('results/estimates/glam_PF2019_nobias_hierarchical_cv'+sufix+'.npy'))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# Save parameter estimates\n",
    "np.save(str('results/estimates/glam_PF2019_nobias_hierarchical_cv'+sufix+'.npy'), glam_nobias.estimates)\n",
    "pd.DataFrame(glam_nobias.estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case it is already fitted\n",
    "params_part_like = pd.DataFrame.from_dict(glam_nobias.estimates.item(0))\n",
    "params_part_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute LOO\n",
    "\n",
    "glam_nobias.loo = pm.loo(trace=glam_nobias.trace, model=glam_nobias.model)\n",
    "glam_nobias.loo\n",
    "\n",
    "np.save(str('results/loo/glam_PF2019_nobias'+ sufix +'.npy'), glam_nobias.loo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "print('Predicting test set data using no-bias GLAM...')\n",
    "glam_nobias.exchange_data(test_data)\n",
    "\n",
    "if not os.path.exists(str('results/predictions/glam_PF2019_nobias_hierarchical_cv'+sufix+'.csv')):\n",
    "    glam_nobias.predict(n_repeats=50)\n",
    "    glam_nobias.prediction.to_csv(str('results/predictions/glam_PF2019_nobias_hierarchical_cv'+sufix+'.csv'), index=False)\n",
    "else:\n",
    "    print('  Found old hierarchical no-bias GLAM predictions in \"results/predictions\". Skipping prediction...')\n",
    "    glam_nobias.prediction = pd.read_csv(str('results/predictions/glam_PF2019_nobias_hierarchical_cv'+sufix+'.csv'))\n",
    "\n",
    "glam_nobias.prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Plot fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close Figure to continue...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-45350e456e2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Close Figure to continue...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mglam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mglam_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#glam.plot_fit(test_data, [glam_full.prediction,glam_nobias.prediction]);\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GiTs/glamDDM_PF2019/glam/plots.py\u001b[0m in \u001b[0;36mplot_fit\u001b[0;34m(data, predictions, color_data)\u001b[0m\n\u001b[1;32m     13\u001b[0m     plot_rt_by_difficulty_zSc(data, predictions,\n\u001b[1;32m     14\u001b[0m                           \u001b[0mxlims\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxlabel_skip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolor_data\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                           ax=axs[0][0])\n\u001b[0m\u001b[1;32m     16\u001b[0m     plot_pleft_by_left_minus_mean_others(data, predictions,\n\u001b[1;32m     17\u001b[0m                                          xlabel_skip=5, xlims=[-3, 3], xlabel_start=0,color1 = color_data, ax=axs[0][1])\n",
      "\u001b[0;32m~/Documents/GiTs/glamDDM_PF2019/glam/plots.py\u001b[0m in \u001b[0;36mplot_rt_by_difficulty_zSc\u001b[0;34m(data, predictions, ax, xlims, xlabel_skip, color1)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscatter_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifficulty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifficulty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mposition_item\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_labels\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0mx_scatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_item\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m## ********\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Close Figure to continue...')\n",
    "glam.plot_fit(test_data, [glam_full.prediction]);\n",
    "#glam.plot_fit(test_data, [glam_full.prediction,glam_nobias.prediction]);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for full hierarchical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 5.1e-05,\n",
       "  'gamma': -0.77,\n",
       "  'SNR': 125.41,\n",
       "  's': 0.0081,\n",
       "  'tau': 0.01,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 1.6e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 290.78,\n",
       "  's': 0.006238,\n",
       "  'tau': 0.05,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 3.5e-05,\n",
       "  'gamma': -0.11,\n",
       "  'SNR': 230.96,\n",
       "  's': 0.008048,\n",
       "  'tau': 0.11,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 2.1e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 434.59,\n",
       "  's': 0.009583,\n",
       "  'tau': 0.1,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 3.2e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 165.04,\n",
       "  's': 0.005638,\n",
       "  'tau': 0.06,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 5.7e-05,\n",
       "  'gamma': -0.71,\n",
       "  'SNR': 83.7,\n",
       "  's': 0.005543,\n",
       "  'tau': 0.09,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 3e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 232.79,\n",
       "  's': 0.008216,\n",
       "  'tau': 0.08,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 5.1e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 99.14,\n",
       "  's': 0.005566,\n",
       "  'tau': 0.09,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 0.00011,\n",
       "  'gamma': -0.86,\n",
       "  'SNR': 49.13,\n",
       "  's': 0.005373,\n",
       "  'tau': 0.0,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 5.7e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 165.72,\n",
       "  's': 0.008702,\n",
       "  'tau': 0.06,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 2.8e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 326.55,\n",
       "  's': 0.008587,\n",
       "  'tau': 0.03,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 9e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 54.42,\n",
       "  's': 0.004899,\n",
       "  'tau': 0.02,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 3.4e-05,\n",
       "  'gamma': -0.56,\n",
       "  'SNR': 86.39,\n",
       "  's': 0.002529,\n",
       "  'tau': 0.0,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 4e-05,\n",
       "  'gamma': -0.87,\n",
       "  'SNR': 232.26,\n",
       "  's': 0.00942,\n",
       "  'tau': 0.25,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 4.7e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 154.16,\n",
       "  's': 0.006544,\n",
       "  'tau': 0.04,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 4.6e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 127.67,\n",
       "  's': 0.007116,\n",
       "  'tau': 0.08,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 1.9e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 476.5,\n",
       "  's': 0.008507,\n",
       "  'tau': 0.03,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 2.8e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 177.22,\n",
       "  's': 0.005505,\n",
       "  'tau': 0.05,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 3.2e-05,\n",
       "  'gamma': -0.87,\n",
       "  'SNR': 340.23,\n",
       "  's': 0.008569,\n",
       "  'tau': 0.04,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 4.5e-05,\n",
       "  'gamma': -0.98,\n",
       "  'SNR': 228.98,\n",
       "  's': 0.008275,\n",
       "  'tau': 0.03,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 3.2e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 213.49,\n",
       "  's': 0.006526,\n",
       "  'tau': 0.08,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 3.6e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 288.26,\n",
       "  's': 0.009493,\n",
       "  'tau': 0.08,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 3e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 232.13,\n",
       "  's': 0.007639,\n",
       "  'tau': 0.07,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 3.7e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 133.67,\n",
       "  's': 0.006737,\n",
       "  'tau': 0.04,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 1.7e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 336.19,\n",
       "  's': 0.007432,\n",
       "  'tau': 0.02,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 3.9e-05,\n",
       "  'gamma': -0.71,\n",
       "  'SNR': 145.41,\n",
       "  's': 0.006367,\n",
       "  'tau': 0.06,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 2.9e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 141.85,\n",
       "  's': 0.004457,\n",
       "  'tau': 0.03,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 4.6e-05,\n",
       "  'gamma': -0.98,\n",
       "  'SNR': 259.96,\n",
       "  's': 0.010083,\n",
       "  'tau': 0.18,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 6.8e-05,\n",
       "  'gamma': -0.99,\n",
       "  'SNR': 62.69,\n",
       "  's': 0.004352,\n",
       "  'tau': 0.11,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 5.7e-05,\n",
       "  'gamma': -0.98,\n",
       "  'SNR': 115.24,\n",
       "  's': 0.008492,\n",
       "  'tau': 0.05,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 7.2e-05,\n",
       "  'gamma': 0.27,\n",
       "  'SNR': 106.53,\n",
       "  's': 0.008452,\n",
       "  'tau': 0.01,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 4.6e-05,\n",
       "  'gamma': -0.94,\n",
       "  'SNR': 145.61,\n",
       "  's': 0.006249,\n",
       "  'tau': 0.01,\n",
       "  't0': array([0.])},\n",
       " {'b': 1.0,\n",
       "  'p_error': 0.05,\n",
       "  'v': 4.4e-05,\n",
       "  'gamma': -0.94,\n",
       "  'SNR': 156.31,\n",
       "  's': 0.007018,\n",
       "  'tau': 0.0,\n",
       "  't0': array([0.])}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_participant = glam_full.estimates\n",
    "params_participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-24a3f3602e89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparams_participant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglam_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mparams_participant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglam_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "params_participant = pd.DataFrame.from_dict(glam_full.estimates.item(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Mean gamma \" +  str(params_participant['gamma'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = params_participant[['SNR','gamma','tau','v']].hist(figsize = [20,3] , layout=[1,4],bins = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [END] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
